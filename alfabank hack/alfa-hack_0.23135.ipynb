{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 13723036,
     "sourceType": "datasetVersion",
     "datasetId": 8730948,
     "isSourceIdPinned": false
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%pip install --no-cache-dir faiss-cpu"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-14T14:06:42.476565Z",
     "iopub.execute_input": "2025-11-14T14:06:42.477110Z",
     "iopub.status.idle": "2025-11-14T14:06:48.029120Z",
     "shell.execute_reply.started": "2025-11-14T14:06:42.477083Z",
     "shell.execute_reply": "2025-11-14T14:06:48.028005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting faiss-cpu\n  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m31.4/31.4 MB\u001B[0m \u001B[31m147.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.12.0\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install --no-cache-dir rank-bm25 snowballstemmer faiss-cpu > /dev/null\n",
    "%pip install --no-cache-dir sentence-transformers > /dev/null"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-14T14:06:48.031141Z",
     "iopub.execute_input": "2025-11-14T14:06:48.031389Z",
     "iopub.status.idle": "2025-11-14T14:07:53.050222Z",
     "shell.execute_reply.started": "2025-11-14T14:06:48.031365Z",
     "shell.execute_reply": "2025-11-14T14:07:53.049271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Note: you may need to restart the kernel to use updated packages.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "pip install -U FlagEmbedding"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-14T14:07:53.051373Z",
     "iopub.execute_input": "2025-11-14T14:07:53.051675Z",
     "iopub.status.idle": "2025-11-14T14:08:15.144399Z",
     "shell.execute_reply.started": "2025-11-14T14:07:53.051630Z",
     "shell.execute_reply": "2025-11-14T14:08:15.143643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting FlagEmbedding\n  Downloading FlagEmbedding-1.3.5.tar.gz (163 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.9/163.9 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.44.2 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (4.53.3)\nRequirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (4.4.1)\nRequirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (1.9.0)\nRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (4.1.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (0.16.0)\nCollecting ir-datasets (from FlagEmbedding)\n  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (6.33.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (7.1.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.3)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (0.36.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets>=2.19.0->FlagEmbedding)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (2025.10.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->FlagEmbedding) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2->FlagEmbedding) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2->FlagEmbedding) (0.21.2)\nRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (4.13.4)\nCollecting inscriptis>=2.2.0 (from ir-datasets->FlagEmbedding)\n  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (5.4.0)\nCollecting trec-car-tools>=2.5.4 (from ir-datasets->FlagEmbedding)\n  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\nCollecting lz4>=3.1.10 (from ir-datasets->FlagEmbedding)\n  Downloading lz4-4.4.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nCollecting warc3-wet>=0.2.3 (from ir-datasets->FlagEmbedding)\n  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\nCollecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->FlagEmbedding)\n  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nCollecting zlib-state>=0.1.3 (from ir-datasets->FlagEmbedding)\n  Downloading zlib_state-0.1.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\nCollecting ijson>=3.1.3 (from ir-datasets->FlagEmbedding)\n  Downloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\nCollecting unlzw3>=0.2.1 (from ir-datasets->FlagEmbedding)\n  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (11.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.7)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.20.1->FlagEmbedding) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2.5.0)\nCollecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding)\n  Downloading cbor-1.0.0.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.19.0->FlagEmbedding) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.20.1->FlagEmbedding) (2024.2.0)\nDownloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m866.1/866.1 kB\u001B[0m \u001B[31m19.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (134 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.4/134.4 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.1/45.1 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading lz4-4.4.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m42.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.7/47.7 MB\u001B[0m \u001B[31m37.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\nDownloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\nDownloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\nDownloading zlib_state-0.1.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (21 kB)\nBuilding wheels for collected packages: FlagEmbedding, warc3-wet-clueweb09, cbor\n  Building wheel for FlagEmbedding (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.3.5-py3-none-any.whl size=233746 sha256=c732fca82ae75ba6308a8e9d7916036a6187e5063c15f1d4a81edd0489f41d8f\n  Stored in directory: /root/.cache/pip/wheels/fc/1c/66/c9c846a8f8cbd9574db8d76b0a61410a087bc07d53682a54f4\n  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=81f88ee213f1e281bafb869a9f7db9d9fb404fe69bc3fb1d406b598532cd4f46\n  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n  Building wheel for cbor (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53931 sha256=cb111f9ba3146d6484d69da675fc709b83e4030b434e47bb76079930e7ad3a1d\n  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\nSuccessfully built FlagEmbedding warc3-wet-clueweb09 cbor\nInstalling collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, pyarrow, lz4, ijson, inscriptis, trec-car-tools, ir-datasets, FlagEmbedding\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed FlagEmbedding-1.3.5 cbor-1.0.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir-datasets-0.5.11 lz4-4.4.5 pyarrow-22.0.0 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# alfabank_kaggle.py\n",
    "# One-file, GPU-optimized retrieval+rerank pipeline for Kaggle\n",
    "# Produces submit.csv with columns: q_id, web_list (JSON array of doc ids)\n",
    "\n",
    "import os, sys, re, math, json, time, csv, logging, pickle, gc\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from typing import List, Dict, Tuple, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit here if needed)\n",
    "# ----------------------------\n",
    "\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "kaggle_path = kagglehub.dataset_download(\"denismuradyan/alfabank-hack\")\n",
    "kaggle_path = Path(kaggle_path)\n",
    "\n",
    "WEBSITES_CSV  = str(kaggle_path / \"websites_updated.csv\")\n",
    "QUESTIONS_CSV = str(kaggle_path / \"questions_clean.csv\")\n",
    "\n",
    "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"artifacts\")\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "BM25_INDEX_PATH = os.path.join(ARTIFACTS_DIR, \"bm25_index.pkl\")\n",
    "\n",
    "# Embeddings model (multi-lingual, strong for RU)\n",
    "EMBEDDING_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "TRUST_REMOTE_CODE = True\n",
    "EMBEDDING_NORMALIZE = True\n",
    "\n",
    "# Token-based chunking\n",
    "CHUNK_TOKENS   = 256\n",
    "OVERLAP_TOKENS = 96\n",
    "\n",
    "# Dense index/search\n",
    "USE_FAISS = True  # FAISS cosine (via inner product on L2-normalized vecs)\n",
    "FAISS_CPU = False  # force CPU for FAISS even if GPU available (debug)\n",
    "BATCH_ADD_SIZE = 256  # how many chunks to embed+add per step\n",
    "\n",
    "# BM25\n",
    "USE_BM25 = True\n",
    "BM25_STEMMING = True\n",
    "BM25_MIN_TOKEN_LEN = 2\n",
    "BM25_NORMALIZE_E = True\n",
    "BM25_SYNONYMS: Dict[str, List[str]] = {\n",
    "    # canonical : variants (stem will apply)\n",
    "    \"кредит\": [\"кредиты\", \"кредитка\", \"карткредит\"],\n",
    "    \"премиум\": [\"a-club\", \"аклуб\", \"a club\", \"премиальный\"],\n",
    "}\n",
    "\n",
    "# Fusion & heuristics\n",
    "WEIGHT_DENSE = 0.5\n",
    "WEIGHT_BM25  = 0.5\n",
    "TITLE_BOOST  = 0.10  # *1.10 if query tokens ∩ title tokens ≠ ∅\n",
    "\n",
    "URL_POSITIVE_PATTERNS = [\n",
    "    r\"/a-?club\", r\"\\bpremium\\b\", r\"wealth\", r\"privilege\", r\"/investment\",\n",
    "]\n",
    "URL_NEGATIVE_PATTERNS = [\n",
    "    r\"/vacanc\", r\"/news\", r\"/press\", r\"/cookies\", r\"/privacy\",\n",
    "]\n",
    "URL_POSITIVE_BOOST   = 0.07\n",
    "URL_NEGATIVE_PENALTY = 0.10\n",
    "\n",
    "# Dynamic weight policy\n",
    "USE_DYNAMIC_WEIGHTS = True\n",
    "SHORT_QUERY_TOKENS  = 5\n",
    "BM25_BONUS_SHORT       = 0.25\n",
    "BM25_BONUS_HAS_DIGITS  = 0.15\n",
    "BM25_BONUS_KEYWORDS    = [\"счет\", \"карта\", \"перевод\", \"платеж\", \"ипотека\", \"кредит\", \"валюта\", \"комисси\"]\n",
    "\n",
    "# Search fanouts / pooling\n",
    "SEARCH_TOP_K_CHUNKS_DENSE = 200\n",
    "SEARCH_TOP_K_CHUNKS_BM25  = 320\n",
    "AGG_TOP_K_PER_DOC         = 6\n",
    "FINAL_TOP_K_DOCS          = 5\n",
    "\n",
    "# Query batch sizes\n",
    "QUERY_EMB_BATCH_SIZE = 256\n",
    "BATCH_ANSWER_CHUNK   = 128   # how many queries per batch in batch_answer()\n",
    "\n",
    "# Cross-encoder reranker\n",
    "USE_RERANKER = True\n",
    "RERANKER_MODEL = \"BAAI/bge-reranker-v2-gemma\"\n",
    "RERANK_CANDIDATE_DOCS = 150\n",
    "RERANKER_BATCH_SIZE   = 32\n",
    "RERANKER_MAX_LENGTH   = 512\n",
    "RERANKER_ALPHA        = 0.7  # blend weight with hybrid score\n",
    "RERANKER_MAX_CHARS_PER_DOC = 3000\n",
    "RERANKER_CHUNKS_PER_DOC    = 4\n",
    "\n",
    "# Throttle on CPU\n",
    "THROTTLE_RERANK_ON_CPU = True\n",
    "RERANK_CANDIDATE_DOCS_CPU = 60\n",
    "RERANKER_BATCH_SIZE_CPU   = 32\n",
    "\n",
    "# Logging/Progress\n",
    "LOG_LEVEL = \"INFO\"\n",
    "SEARCH_LOG_EVERY = 16\n",
    "\n",
    "# Output\n",
    "SUBMIT_PATH = os.path.join(PROJECT_ROOT, \"submit.csv\")\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# Logging setup\n",
    "# ---------------\n",
    "logger = logging.getLogger(\"run\")\n",
    "if not logger.handlers:\n",
    "    hh = logging.StreamHandler(sys.stdout)\n",
    "    hh.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))\n",
    "    logger.addHandler(hh)\n",
    "logger.setLevel(getattr(logging, LOG_LEVEL.upper(), logging.INFO))\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Robust CSV reading\n",
    "# -------------------\n",
    "def _bump_field_size_limit():\n",
    "    max_int = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(max_int)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            max_int = int(max_int / 10)\n",
    "\n",
    "def _read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    encs = [\"utf-8\", \"utf-8-sig\", \"cp1251\", \"windows-1251\", \"latin1\"]\n",
    "    last = None\n",
    "    for e in encs:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\",\", engine=\"c\", encoding=e, quoting=csv.QUOTE_MINIMAL)\n",
    "        except Exception as ex:\n",
    "            last = ex\n",
    "    _bump_field_size_limit()\n",
    "    for e in encs:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=None, engine=\"python\", encoding=e, on_bad_lines=\"skip\")\n",
    "        except Exception as ex:\n",
    "            last = ex\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return pd.read_csv(StringIO(f.read()), sep=\",\", engine=\"c\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
    "            return pd.read_csv(StringIO(f.read()), sep=\",\", engine=\"c\")\n",
    "    except Exception:\n",
    "        raise last if last else RuntimeError(f\"Failed to read CSV: {path}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Preprocessor & chunks\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    web_id: int\n",
    "    document_id: int\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "    chunk_idx: int\n",
    "    is_title_chunk: bool\n",
    "\n",
    "def _clean_text(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def _rename_like(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    low = {c: c.strip().lower() for c in df.columns}\n",
    "    df = df.rename(columns=low)\n",
    "\n",
    "    def pick(cols, candidates):\n",
    "        for cand in candidates:\n",
    "            if cand in cols:\n",
    "                return cand\n",
    "        return None\n",
    "\n",
    "    cols = set(df.columns)\n",
    "    # websites\n",
    "    if \"web_id\" not in cols:\n",
    "        alt = pick(cols, [\"id\", \"page_id\", \"doc_id\"])\n",
    "        if alt: df = df.rename(columns={alt: \"web_id\"})\n",
    "    if \"url\" not in cols:\n",
    "        alt = pick(cols, [\"link\", \"href\"])\n",
    "        if alt: df = df.rename(columns={alt: \"url\"})\n",
    "    if \"title\" not in cols:\n",
    "        alt = pick(cols, [\"page_title\", \"name\", \"header\"])\n",
    "        if alt: df = df.rename(columns={alt: \"title\"})\n",
    "    if \"text\" not in cols:\n",
    "        alt = pick(cols, [\"content\", \"body\", \"parsed_text\", \"html_text\"])\n",
    "        if alt: df = df.rename(columns={alt: \"text\"})\n",
    "    # questions\n",
    "    if \"q_id\" not in cols:\n",
    "        alt = pick(cols, [\"id\", \"question_id\"])\n",
    "        if alt: df = df.rename(columns={alt: \"q_id\"})\n",
    "    if \"query\" not in cols:\n",
    "        alt = pick(cols, [\"question\", \"q_text\", \"text\"])\n",
    "        if alt: df = df.rename(columns={alt: \"query\"})\n",
    "    return df\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "class TokenChunker:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, use_fast=True, trust_remote_code=TRUST_REMOTE_CODE\n",
    "        )\n",
    "        self.chunk_tokens = CHUNK_TOKENS\n",
    "        self.overlap_tokens = OVERLAP_TOKENS\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        if not text: return []\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks, step = [], (self.chunk_tokens - self.overlap_tokens)\n",
    "        for start in range(0, len(tokens), step):\n",
    "            piece = tokens[start:start+self.chunk_tokens]\n",
    "            if not piece: break\n",
    "            chunks.append(self.tokenizer.decode(piece, skip_special_tokens=True))\n",
    "            if start + self.chunk_tokens >= len(tokens):\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "def load_corpus() -> pd.DataFrame:\n",
    "    df = _read_csv_robust(WEBSITES_CSV)\n",
    "    df = _rename_like(df)\n",
    "    need = [\"web_id\", \"url\", \"title\", \"text\"]\n",
    "    for col in need:\n",
    "        if col not in df.columns:\n",
    "            if col in (\"title\", \"text\"):\n",
    "                df[col] = \"\"\n",
    "            else:\n",
    "                raise ValueError(f\"websites CSV missing column '{col}'\")\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "    return df[need]\n",
    "\n",
    "def load_queries() -> pd.DataFrame:\n",
    "    dfq = _read_csv_robust(QUESTIONS_CSV)\n",
    "    dfq = _rename_like(dfq)\n",
    "    if \"q_id\" not in dfq.columns or \"query\" not in dfq.columns:\n",
    "        raise ValueError(\"questions CSV must contain q_id and query\")\n",
    "    dfq[\"query\"] = dfq[\"query\"].fillna(\"\")\n",
    "    return dfq[[\"q_id\", \"query\"]]\n",
    "\n",
    "def create_chunks_from_websites(df_web: pd.DataFrame, model_name: str) -> List[Chunk]:\n",
    "    tk = TokenChunker(model_name)\n",
    "    chunks: List[Chunk] = []\n",
    "    for row in df_web.itertuples(index=False):\n",
    "        web_id = int(getattr(row, \"web_id\"))\n",
    "        url   = _clean_text(getattr(row, \"url\", \"\"))\n",
    "        title = _clean_text(getattr(row, \"title\", \"\"))\n",
    "        body  = _clean_text(getattr(row, \"text\", \"\"))\n",
    "\n",
    "        lead = body[:500] if body else \"\"\n",
    "        title_payload = (title + \" \" + lead).strip() if (title or lead) else \"\"\n",
    "        idx = 0\n",
    "        if title_payload:\n",
    "            chunks.append(Chunk(\n",
    "                chunk_id=f\"{web_id}::t0\", web_id=web_id, document_id=web_id,\n",
    "                url=url, title=title, text=title_payload, chunk_idx=idx, is_title_chunk=True\n",
    "            ))\n",
    "            idx += 1\n",
    "        for piece in tk.chunk_text(body):\n",
    "            chunks.append(Chunk(\n",
    "                chunk_id=f\"{web_id}::{idx}\", web_id=web_id, document_id=web_id,\n",
    "                url=url, title=title, text=piece, chunk_idx=idx, is_title_chunk=False\n",
    "            ))\n",
    "            idx += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Embedder (GPU)\n",
    "# -----------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "_E5_LIKE_PATTERNS = [r\"\\be5\\b\", r\"\\bgte\\b\", r\"\\bbge-m3\\b\", r\"\\b(text-)?embedding\\b\"]\n",
    "def _is_e5_like(model_name: str) -> bool:\n",
    "    name = model_name.lower()\n",
    "    return any(re.search(p, name) for p in _E5_LIKE_PATTERNS)\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: Optional[str] = None, normalize: Optional[bool] = None):\n",
    "        self.model_name = model_name or EMBEDDING_MODEL\n",
    "        self.normalize  = EMBEDDING_NORMALIZE if normalize is None else normalize\n",
    "        self._use_e5_prefix = _is_e5_like(self.model_name)\n",
    "        self.model = SentenceTransformer(self.model_name, trust_remote_code=TRUST_REMOTE_CODE, device=None)\n",
    "\n",
    "    def _maybe_prefix_queries(self, queries: List[str]) -> List[str]:\n",
    "        if not self._use_e5_prefix: return queries\n",
    "        return [f\"query: {q}\" for q in queries]\n",
    "\n",
    "    def _maybe_prefix_passages(self, passages: List[str]) -> List[str]:\n",
    "        if not self._use_e5_prefix: return passages\n",
    "        return [f\"passage: {p}\" for p in passages]\n",
    "\n",
    "    def encode_queries(self, queries: List[str], batch_size: int = QUERY_EMB_BATCH_SIZE) -> np.ndarray:\n",
    "        texts = self._maybe_prefix_queries(queries)\n",
    "        emb = self.model.encode(texts, batch_size=batch_size, convert_to_numpy=True,\n",
    "                                show_progress_bar=False, normalize_embeddings=self.normalize)\n",
    "        return emb.astype(np.float32)\n",
    "\n",
    "    def encode_passages(self, passages: List[str], batch_size: int = 256) -> np.ndarray:\n",
    "        texts = self._maybe_prefix_passages(passages)\n",
    "        emb = self.model.encode(texts, batch_size=batch_size, convert_to_numpy=True,\n",
    "                                show_progress_bar=True, normalize_embeddings=self.normalize)\n",
    "        return emb.astype(np.float32)\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# BM25 over chunks\n",
    "# ---------------\n",
    "from rank_bm25 import BM25Okapi\n",
    "try:\n",
    "    import snowballstemmer  # type: ignore\n",
    "    _STEMMER = snowballstemmer.stemmer(\"russian\") if BM25_STEMMING else None\n",
    "except Exception:\n",
    "    _STEMMER = None\n",
    "\n",
    "_WORD_RE = re.compile(r\"[A-Za-zА-Яа-яЁё0-9]+\")\n",
    "\n",
    "def _normalize_token(t: str) -> str:\n",
    "    t = t.lower()\n",
    "    if BM25_NORMALIZE_E:\n",
    "        t = t.replace(\"ё\", \"е\")\n",
    "    return t\n",
    "\n",
    "def _stem(t: str) -> str:\n",
    "    if not _STEMMER: return t\n",
    "    return _STEMMER.stemWord(t)\n",
    "\n",
    "def _tokenize_ru(text: str) -> List[str]:\n",
    "    if not text: return []\n",
    "    toks = _WORD_RE.findall(text)\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        tt = _normalize_token(t)\n",
    "        if len(tt) < BM25_MIN_TOKEN_LEN: continue\n",
    "        out.append(_stem(tt))\n",
    "    return out\n",
    "\n",
    "def _expand_query_tokens(tokens: List[str]) -> List[str]:\n",
    "    if not BM25_SYNONYMS: return tokens\n",
    "    bag = set(tokens)\n",
    "    for canon, variants in BM25_SYNONYMS.items():\n",
    "        all_forms = [canon] + variants\n",
    "        norm_forms = [_stem(_normalize_token(x)) for x in all_forms]\n",
    "        if any(f in bag for f in norm_forms):\n",
    "            bag.update(norm_forms)\n",
    "    return list(bag)\n",
    "\n",
    "@dataclass\n",
    "class _ChunkLite:\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "\n",
    "class BM25ChunkIndex:\n",
    "    def __init__(self, tokenized_corpus: List[List[str]], chunks: List[_ChunkLite]):\n",
    "        self._bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self._chunk_ids = [c.chunk_id for c in chunks]\n",
    "\n",
    "    @classmethod\n",
    "    def build_from_chunks(cls, chunks_full: Iterable[Chunk]) -> \"BM25ChunkIndex\":\n",
    "        chunks = [_ChunkLite(c.chunk_id, c.text) for c in chunks_full]\n",
    "        tokenized = [_tokenize_ru(c.text) for c in chunks]\n",
    "        return cls(tokenized, chunks)\n",
    "\n",
    "    def query(self, query: str, top_k: int = 100) -> List[Tuple[str, float]]:\n",
    "        q_tokens = _expand_query_tokens(_tokenize_ru(query))\n",
    "        scores = self._bm25.get_scores(q_tokens)\n",
    "        order = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(self._chunk_ids[int(i)], float(scores[int(i)])) for i in order]\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump({\"bm25\": self._bm25, \"chunk_ids\": self._chunk_ids}, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"BM25ChunkIndex\":\n",
    "        with open(path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        inst = object.__new__(cls)\n",
    "        inst._bm25 = obj[\"bm25\"]\n",
    "        inst._chunk_ids = obj[\"chunk_ids\"]\n",
    "        return inst\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# FAISS dense index/search\n",
    "# ------------------------\n",
    "def _build_faiss(emb_dim: int):\n",
    "    try:\n",
    "        import faiss\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"FAISS not available, falling back to NumPy search ({e})\")\n",
    "        return None, None\n",
    "\n",
    "    use_gpu = False\n",
    "    if not FAISS_CPU:\n",
    "        # Try GPU\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            use_gpu = True\n",
    "        except Exception:\n",
    "            use_gpu = False\n",
    "\n",
    "    index = faiss.IndexFlatIP(emb_dim)\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            index = faiss.index_cpu_to_all_gpus(index)\n",
    "            logger.info(\"FAISS: using GPU index\")\n",
    "        except Exception:\n",
    "            logger.info(\"FAISS: GPU transfer failed, using CPU\")\n",
    "    else:\n",
    "        logger.info(\"FAISS: using CPU index\")\n",
    "\n",
    "    return index, use_gpu\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Cross-Encoder CE\n",
    "# --------------------\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from FlagEmbedding import FlagLLMReranker\n",
    "\n",
    "class CrossEncoderReranker:\n",
    "    def __init__(self, prefer: Optional[str] = None):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # здесь prefer/RERANKER_MODEL можно игнорировать, жёстко берём gemma\n",
    "        self.model = FlagLLMReranker(\n",
    "            \"BAAI/bge-reranker-v2-gemma\",\n",
    "            use_fp16=True  # на GPU, чтобы не умереть по времени/памяти\n",
    "        )\n",
    "        logger.info(f\"[RERANKER] loaded: BAAI/bge-reranker-v2-gemma on {device}\")\n",
    "\n",
    "    def score(self, query: str, docs: List[str], batch_size: Optional[int] = None) -> List[float]:\n",
    "        if not docs:\n",
    "            return []\n",
    "        pairs = [[query, d] for d in docs]\n",
    "        # FlagLLMReranker сам батает пары внутри, batch_size можно игнорировать\n",
    "        scores = self.model.compute_score(pairs)  # список float\n",
    "        return [float(s) for s in scores]\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Retrieval pipeline\n",
    "# -------------------\n",
    "def re_split_simple(text: str) -> List[str]:\n",
    "    return _WORD_RE.findall(text or \"\")\n",
    "\n",
    "def _min_max_norm(scores: Dict[int, float]) -> Dict[int, float]:\n",
    "    if not scores: return {}\n",
    "    vals = list(scores.values())\n",
    "    vmin, vmax = min(vals), max(vals)\n",
    "    if math.isclose(vmin, vmax): return {k: 0.0 for k in scores}\n",
    "    return {k: (v - vmin) / (vmax - vmin) for k, v in scores.items()}\n",
    "\n",
    "_POS_PATTS = [re.compile(p, re.I) for p in URL_POSITIVE_PATTERNS]\n",
    "_NEG_PATTS = [re.compile(p, re.I) for p in URL_NEGATIVE_PATTERNS]\n",
    "def _pos_url_title_bonus(url: str, title: str) -> float:\n",
    "    s = url + \" \" + (title or \"\")\n",
    "    return URL_POSITIVE_BOOST if any(p.search(s) for p in _POS_PATTS) else 0.0\n",
    "def _neg_url_penalty(url: str) -> float:\n",
    "    return URL_NEGATIVE_PENALTY if any(p.search(url) for p in _NEG_PATTS) else 0.0\n",
    "\n",
    "def _is_short_query(q: str) -> bool:\n",
    "    toks = [t for t in re_split_simple(q) if len(t) > 2]\n",
    "    return len(toks) <= SHORT_QUERY_TOKENS\n",
    "def _has_digits(q: str) -> bool:\n",
    "    return any(ch.isdigit() for ch in q)\n",
    "def _has_keywords(q: str) -> bool:\n",
    "    low = q.lower()\n",
    "    return any(kw in low for kw in BM25_BONUS_KEYWORDS)\n",
    "\n",
    "def _dynamic_weights(q: str) -> Tuple[float, float]:\n",
    "    d, b = WEIGHT_DENSE, WEIGHT_BM25\n",
    "    if not USE_DYNAMIC_WEIGHTS:\n",
    "        return d, b\n",
    "    bonus = 0.0\n",
    "    if _is_short_query(q): bonus += BM25_BONUS_SHORT\n",
    "    if _has_digits(q):     bonus += BM25_BONUS_HAS_DIGITS\n",
    "    if _has_keywords(q):   bonus += 0.15\n",
    "    b_new = min(0.9, max(0.1, b + bonus))\n",
    "    d_new = 1.0 - b_new\n",
    "    return d_new, b_new\n",
    "\n",
    "def _sentence_split(text: str) -> List[str]:\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text)\n",
    "    return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "def _sent_score(sent: str, q_tokens: set[str]) -> float:\n",
    "    toks = {t.lower() for t in re_split_simple(sent) if len(t) > 2}\n",
    "    return float(len(toks & q_tokens))\n",
    "\n",
    "class OneFileRetrieval:\n",
    "    def __init__(self):\n",
    "        self.embedder = Embedder()\n",
    "        self._bm25: Optional[BM25ChunkIndex] = None\n",
    "        self._faiss_index = None\n",
    "        self._faiss_use_gpu = None\n",
    "        self._chunk_meta: Dict[str, Chunk] = {}\n",
    "        self._doc_info: Dict[int, Tuple[str, str]] = {}\n",
    "        self._chunk_ids: List[str] = []  # order matches FAISS ids\n",
    "        self._reranker = CrossEncoderReranker() if USE_RERANKER else None\n",
    "\n",
    "    def build_index(self) -> None:\n",
    "        logger.info(\"Loading corpus...\")\n",
    "        df_web = load_corpus()\n",
    "        logger.info(f\"Corpus: {len(df_web)} pages\")\n",
    "        logger.info(\"Chunking pages (token-based)...\")\n",
    "        chunks = create_chunks_from_websites(df_web, model_name=EMBEDDING_MODEL)\n",
    "        logger.info(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "        self._chunk_meta = {c.chunk_id: c for c in chunks}\n",
    "        for c in chunks:\n",
    "            if c.document_id not in self._doc_info:\n",
    "                self._doc_info[c.document_id] = (c.url, c.title)\n",
    "\n",
    "        # Dense embeddings for chunks (GPU)\n",
    "        logger.info(\"Encoding chunk embeddings (dense)...\")\n",
    "        docs_all = [c.text for c in chunks]\n",
    "        emb_all = []\n",
    "        t0 = time.time()\n",
    "        for start in range(0, len(docs_all), BATCH_ADD_SIZE):\n",
    "            end = min(start + BATCH_ADD_SIZE, len(docs_all))\n",
    "            emb = self.embedder.encode_passages(docs_all[start:end], batch_size=256)\n",
    "            emb_all.append(emb)\n",
    "            logger.info(f\"  chunks {start}..{end-1} done (elapsed {time.time()-t0:.1f}s)\")\n",
    "        emb_all = np.vstack(emb_all).astype(np.float32)\n",
    "        logger.info(f\"Embeddings ready: {emb_all.shape} in {time.time()-t0:.1f}s\")\n",
    "\n",
    "        # FAISS index (cosine via IP on normalized)\n",
    "        self._faiss_index, self._faiss_use_gpu = _build_faiss(emb_all.shape[1])\n",
    "        if self._faiss_index is None:\n",
    "            # fallback: store for NumPy search\n",
    "            self._emb_matrix = emb_all\n",
    "            logger.info(\"Using NumPy dense search fallback.\")\n",
    "        else:\n",
    "            self._faiss_index.add(emb_all)\n",
    "        self._chunk_ids = [c.chunk_id for c in chunks]\n",
    "        del emb_all; gc.collect()\n",
    "\n",
    "        # BM25 over chunks\n",
    "        if USE_BM25:\n",
    "            logger.info(\"Building BM25 index over chunks...\")\n",
    "            self._bm25 = BM25ChunkIndex.build_from_chunks(chunks)\n",
    "            self._bm25.save(BM25_INDEX_PATH)\n",
    "            logger.info(f\"BM25 saved to {BM25_INDEX_PATH}\")\n",
    "\n",
    "        logger.info(\"Index build completed.\")\n",
    "\n",
    "    # Dense candidates for a batch of queries\n",
    "    def _dense_candidates_batch(self, q_vecs: np.ndarray, n_results: int) -> List[List[Tuple[str, float]]]:\n",
    "        if getattr(self, \"_faiss_index\", None) is None:\n",
    "            # NumPy fallback\n",
    "            sims = np.dot(q_vecs, self._emb_matrix.T)  # (B, N)\n",
    "            idx = np.argsort(-sims, axis=1)[:, :n_results]\n",
    "            out = []\n",
    "            for i in range(q_vecs.shape[0]):\n",
    "                ids = idx[i]\n",
    "                pairs = [(self._chunk_ids[j], float(sims[i, j])) for j in ids]\n",
    "                out.append(pairs)\n",
    "            return out\n",
    "\n",
    "        # FAISS path\n",
    "        import faiss  # type: ignore\n",
    "        D, I = self._faiss_index.search(q_vecs, n_results)  # sims & indices\n",
    "        out = []\n",
    "        for i in range(q_vecs.shape[0]):\n",
    "            pairs = []\n",
    "            for d, j in zip(D[i], I[i]):\n",
    "                if j < 0: continue\n",
    "                pairs.append((self._chunk_ids[int(j)], float(d)))\n",
    "            out.append(pairs)\n",
    "        return out\n",
    "\n",
    "    def _topk_pool_per_doc(\n",
    "        self, pairs: List[Tuple[str, float]], k: int\n",
    "    ) -> Tuple[Dict[int, float], Dict[int, List[Tuple[str, float]]]]:\n",
    "        per_doc: Dict[int, List[Tuple[str, float]]] = {}\n",
    "        for chunk_id, score in pairs:\n",
    "            ch = self._chunk_meta.get(chunk_id)\n",
    "            if not ch: continue\n",
    "            per_doc.setdefault(ch.document_id, []).append((chunk_id, float(score)))\n",
    "        doc_scores: Dict[int, float] = {}\n",
    "        doc_chunks: Dict[int, List[Tuple[str, float]]] = {}\n",
    "        for doc_id, vals in per_doc.items():\n",
    "            vals.sort(key=lambda x: x[1], reverse=True)\n",
    "            pool = vals[:k]\n",
    "            if pool:\n",
    "                smax = pool[0][1]\n",
    "                mean_k = float(np.mean([s for _, s in pool]))\n",
    "                doc_scores[doc_id] = float(smax + 0.5 * mean_k)\n",
    "                doc_chunks[doc_id] = pool\n",
    "            else:\n",
    "                doc_scores[doc_id] = 0.0\n",
    "                doc_chunks[doc_id] = []\n",
    "        return doc_scores, doc_chunks\n",
    "\n",
    "    def _apply_domain_boosts(self, scores: Dict[int, float], q_text: str) -> Dict[int, float]:\n",
    "        if not scores: return scores\n",
    "        out = {}\n",
    "        q_low = q_text.lower()\n",
    "        is_premium = any(x in q_low for x in [\"a-club\", \"аклуб\", \"премиум\", \"private\", \"wealth\"])\n",
    "        for doc_id, s in scores.items():\n",
    "            url, title = self._doc_info.get(doc_id, (\"\", \"\"))\n",
    "            bonus = _pos_url_title_bonus(url, title)\n",
    "            malus = 0.0 if is_premium else _neg_url_penalty(url)\n",
    "            out[doc_id] = s * (1.0 + bonus) * (1.0 - malus)\n",
    "        return out\n",
    "\n",
    "    def _build_doc_text_for_ce(\n",
    "        self, q_text: str, doc_id: int, dense_chunks: Dict[int, List[Tuple[str, float]]],\n",
    "        bm25_chunks: Dict[int, List[Tuple[str, float]]]\n",
    "    ) -> str:\n",
    "        title = self._doc_info.get(doc_id, (\"\", \"\"))[1] or \"\"\n",
    "        merged: List[Tuple[str, float]] = []\n",
    "        for dct in (dense_chunks, bm25_chunks):\n",
    "            if doc_id in dct:\n",
    "                merged.extend(dct[doc_id])\n",
    "        seen = set()\n",
    "        merged = sorted(merged, key=lambda x: x[1], reverse=True)\n",
    "        uniq: List[Tuple[str, float]] = []\n",
    "        for cid, s in merged:\n",
    "            if cid not in seen:\n",
    "                uniq.append((cid, s)); seen.add(cid)\n",
    "\n",
    "        parts: List[str] = []\n",
    "        q_tokens = {t.lower() for t in re_split_simple(q_text) if len(t) > 2}\n",
    "        for cid, _ in uniq[:RERANKER_CHUNKS_PER_DOC]:\n",
    "            ch = self._chunk_meta.get(cid)\n",
    "            if not ch: continue\n",
    "            sents = _sentence_split(ch.text)\n",
    "            scored = sorted(sents, key=lambda st: _sent_score(st, q_tokens), reverse=True)\n",
    "            take = max(1, min(3, len(scored)))\n",
    "            parts.extend(scored[:take])\n",
    "\n",
    "        text_blob = (title + \"\\n\\n\" + \"\\n\".join(parts)).strip()\n",
    "        if len(text_blob) > RERANKER_MAX_CHARS_PER_DOC:\n",
    "            text_blob = text_blob[:RERANKER_MAX_CHARS_PER_DOC]\n",
    "        return text_blob\n",
    "\n",
    "    def search_batch(self, queries: List[str], top_k: int) -> List[List[int]]:\n",
    "        if not queries: return []\n",
    "        t0 = time.time()\n",
    "        logger.info(f\"[search_batch] start: {len(queries)} queries\")\n",
    "\n",
    "        # Encode queries\n",
    "        qq = self.embedder.encode_queries(queries, batch_size=QUERY_EMB_BATCH_SIZE)\n",
    "        logger.info(f\"[search_batch] embedded {len(queries)} q in {time.time()-t0:.2f}s\")\n",
    "\n",
    "        # Dense candidates\n",
    "        t1 = time.time()\n",
    "        dense_pairs_list = self._dense_candidates_batch(qq, n_results=SEARCH_TOP_K_CHUNKS_DENSE)\n",
    "        logger.info(f\"[search_batch] dense candidates in {time.time()-t1:.2f}s\")\n",
    "\n",
    "        # Rerank throttling on CPU\n",
    "        local_rerank_k = RERANK_CANDIDATE_DOCS\n",
    "        local_bs = RERANKER_BATCH_SIZE\n",
    "        if THROTTLE_RERANK_ON_CPU and not torch.cuda.is_available():\n",
    "            local_rerank_k = min(local_rerank_k, RERANK_CANDIDATE_DOCS_CPU)\n",
    "            local_bs = max(1, RERANKER_BATCH_SIZE_CPU)\n",
    "            logger.info(f\"[search_batch] CPU detected → cand={local_rerank_k}, bs={local_bs}\")\n",
    "\n",
    "        results = []\n",
    "        processed = 0\n",
    "        every = max(1, SEARCH_LOG_EVERY)\n",
    "\n",
    "        for i, (q_text, pairs) in enumerate(zip(queries, dense_pairs_list), 1):\n",
    "            t_q = time.time()\n",
    "            w_dense, w_bm25 = _dynamic_weights(q_text)\n",
    "\n",
    "            dense_doc, dense_chunks = self._topk_pool_per_doc(pairs, k=AGG_TOP_K_PER_DOC)\n",
    "\n",
    "            bm25_doc: Dict[int, float] = {}\n",
    "            bm25_chunks: Dict[int, List[Tuple[str, float]]] = {}\n",
    "            if USE_BM25 and self._bm25 is not None:\n",
    "                add = 150 if (_is_short_query(q_text) or _has_keywords(q_text)) else 0\n",
    "                bm25_pairs = self._bm25.query(q_text, top_k=SEARCH_TOP_K_CHUNKS_BM25 + add)\n",
    "                bm25_doc, bm25_chunks = self._topk_pool_per_doc(bm25_pairs, k=AGG_TOP_K_PER_DOC)\n",
    "\n",
    "            dense_norm = _min_max_norm(dense_doc)\n",
    "            bm25_norm  = _min_max_norm(bm25_doc) if USE_BM25 else {}\n",
    "            all_docs   = set(dense_norm.keys()) | set(bm25_norm.keys())\n",
    "            hybrid_scores = {\n",
    "                d: (w_dense * dense_norm.get(d, 0.0) + w_bm25 * bm25_norm.get(d, 0.0))\n",
    "                for d in all_docs\n",
    "            }\n",
    "\n",
    "            if TITLE_BOOST > 0:\n",
    "                q_tokens = {t.lower() for t in re_split_simple(q_text) if len(t) > 2}\n",
    "                for doc_id, (_, title) in self._doc_info.items():\n",
    "                    title_tokens = {t.lower() for t in re_split_simple(title) if len(t) > 2}\n",
    "                    if q_tokens & title_tokens and doc_id in hybrid_scores:\n",
    "                        hybrid_scores[doc_id] *= (1.0 + TITLE_BOOST)\n",
    "\n",
    "            hybrid_scores = self._apply_domain_boosts(hybrid_scores, q_text)\n",
    "            prelim_ranked = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            prelim_docs = [doc for doc, _ in prelim_ranked]\n",
    "\n",
    "            # Cross-Encoder rerank\n",
    "            if USE_RERANKER and (self._reranker is not None):\n",
    "                cand_docs = prelim_docs[:local_rerank_k]\n",
    "                cand_texts = [self._build_doc_text_for_ce(q_text, d, dense_chunks, bm25_chunks) for d in cand_docs]\n",
    "                ce_scores = self._reranker.score(q_text, cand_texts, batch_size=local_bs)\n",
    "                if ce_scores:\n",
    "                    ce_min, ce_max = float(min(ce_scores)), float(max(ce_scores))\n",
    "                    ce_norm = [0.0 if math.isclose(ce_min, ce_max) else (s - ce_min) / (ce_max - ce_min) for s in ce_scores]\n",
    "                    mix_scores: Dict[int, float] = {}\n",
    "                    for d, base_s, ce_s in zip(cand_docs, [hybrid_scores.get(d, 0.0) for d in cand_docs], ce_norm):\n",
    "                        mix_scores[d] = (1.0 - RERANKER_ALPHA) * base_s + RERANKER_ALPHA * ce_s\n",
    "                    final_ranked = sorted(mix_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "                    top_docs = [doc for doc, _ in final_ranked[:top_k]]\n",
    "                else:\n",
    "                    top_docs = prelim_docs[:top_k]\n",
    "            else:\n",
    "                top_docs = prelim_docs[:top_k]\n",
    "\n",
    "            # pad/unique to top_k\n",
    "            if len(top_docs) < top_k:\n",
    "                seen = set(top_docs)\n",
    "                for d in prelim_docs:\n",
    "                    if d not in seen:\n",
    "                        top_docs.append(d); seen.add(d)\n",
    "                    if len(top_docs) == top_k: break\n",
    "\n",
    "            uniq_docs = []\n",
    "            for d in top_docs:\n",
    "                if d not in uniq_docs:\n",
    "                    uniq_docs.append(d)\n",
    "                if len(uniq_docs) == top_k: break\n",
    "\n",
    "            results.append(uniq_docs)\n",
    "            processed += 1\n",
    "\n",
    "            if processed == 1 or processed % every == 0 or processed == len(queries):\n",
    "                elapsed = time.time() - t0\n",
    "                rate = processed / max(elapsed, 1e-6)\n",
    "                eta  = (len(queries) - processed) / max(rate, 1e-6)\n",
    "                logger.info(f\"[search_batch] {processed}/{len(queries)} | last {time.time()-t_q:.2f}s | \"\n",
    "                            f\"avg {elapsed/processed:.2f}s/q | ETA {eta/60:.1f} min\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def batch_answer(self, dfq: pd.DataFrame) -> pd.DataFrame:\n",
    "        n = len(dfq)\n",
    "        if n == 0:\n",
    "            return pd.DataFrame(columns=[\"q_id\", \"web_list\"])\n",
    "        logger.info(f\"[batch_answer] Start answering {n} queries...\")\n",
    "        t0 = time.time()\n",
    "        rows = []\n",
    "        chunk = max(1, BATCH_ANSWER_CHUNK)\n",
    "        for i_start in range(0, n, chunk):\n",
    "            i_end = min(i_start + chunk, n)\n",
    "            part = dfq.iloc[i_start:i_end]\n",
    "            queries = part[\"query\"].tolist()\n",
    "            q_ids   = part[\"q_id\"].astype(int).tolist()\n",
    "            t_chunk = time.time()\n",
    "            docs_lists = self.search_batch(queries, top_k=FINAL_TOP_K_DOCS)\n",
    "            dur = time.time() - t_chunk\n",
    "            for q_id, docs in zip(q_ids, docs_lists):\n",
    "                while len(docs) < FINAL_TOP_K_DOCS:\n",
    "                    docs.append(docs[-1] if docs else 1)\n",
    "                rows.append({\"q_id\": q_id, \"web_list\": docs})\n",
    "            done = i_end\n",
    "            elapsed = time.time() - t0\n",
    "            rate = done / max(elapsed, 1e-6)\n",
    "            eta  = (n - done) / max(rate, 1e-6)\n",
    "            logger.info(f\"[batch_answer] {done}/{n} | chunk {i_end-i_start} in {dur:.2f}s | \"\n",
    "                        f\"avg {elapsed/done:.2f}s/q | {rate:.2f} q/s | ETA {eta/60:.1f} min\")\n",
    "        logger.info(f\"[batch_answer] Finished {n} queries in {time.time()-t0:.1f}s\")\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# -----------\n",
    "# Main\n",
    "# -----------\n",
    "def main():\n",
    "    print(\"PROJECT_ROOT =\", PROJECT_ROOT, flush=True)\n",
    "    print(\"WEBSITES_CSV =\", WEBSITES_CSV, flush=True)\n",
    "    print(\"QUESTIONS_CSV =\", QUESTIONS_CSV, flush=True)\n",
    "\n",
    "    retr = OneFileRetrieval()\n",
    "    t0 = time.time()\n",
    "    retr.build_index()\n",
    "    print(f\"[INFO] Index ready in {time.time()-t0:.1f}s. Loading queries...\", flush=True)\n",
    "\n",
    "    dfq = load_queries()\n",
    "    print(f\"[INFO] Queries loaded: {len(dfq)}. Starting answering...\", flush=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    out = retr.batch_answer(dfq)\n",
    "    print(f\"[INFO] Answering done in {time.time()-t1:.1f}s. Saving submit.csv ...\", flush=True)\n",
    "\n",
    "    out[\"web_list\"] = out[\"web_list\"].apply(lambda xs: json.dumps(xs, ensure_ascii=False))\n",
    "    out.to_csv(SUBMIT_PATH, index=False)\n",
    "    print(f\"submit.csv saved → {SUBMIT_PATH}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-14T14:08:15.146157Z",
     "iopub.execute_input": "2025-11-14T14:08:15.146394Z",
     "iopub.status.idle": "2025-11-14T14:10:45.328742Z",
     "shell.execute_reply.started": "2025-11-14T14:08:15.146369Z",
     "shell.execute_reply": "2025-11-14T14:10:45.327339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-11-14 14:08:28.465318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763129308.658507      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763129308.709071      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "name": "stdout",
     "text": "PROJECT_ROOT = /kaggle/working\nWEBSITES_CSV = /kaggle/input/alfabank-hack/websites_updated.csv\nQUESTIONS_CSV = /kaggle/input/alfabank-hack/questions_clean.csv\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55c9fe5eab8f437e99ef43c017926ed4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "570728b4b1ed4592b7c6305b359b8528"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c74950fc5e9480caf72d21797afa526"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fc28a2104754458b8fca8423fc90368"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7fa118e8ecd40259872a4f20210d467"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acea333d913646a096b17b7ca35844a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6db4def6287c408581d8a43cacf809f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c536ea8748b14d15a144abe6bac464c4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99157c59c50044e49634c68fa7d2943c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "048333f99fa34239bc26013d52f489a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d19cbef6064451faf057d807df4fa4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6731c2e330424e429c93717cf896af6f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3ce22b40b0f4bf8844379911419fdb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b560010b861540a982037b020e4904b9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d40036683b04388ae2e92138bd5d2d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c78c7ade16b7425f9ae317c7b7c9d904"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ed8f591e90d4269a883f5e6f1e77724"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00003.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9de8e3cf56184eb0afcdaed12416dbc0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00003-of-00003.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0646a0d2a7dd4e118dace8ed9ad34db6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dcaa253b13b4f988a8a28c7c2340242"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a393b2c390443978a86e2fdb10d5ca7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d65800c066441538d7d0aafaa083076"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO] [RERANKER] loaded: BAAI/bge-reranker-v2-gemma on cuda\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:[RERANKER] loaded: BAAI/bge-reranker-v2-gemma on cuda\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Loading corpus...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Loading corpus...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Corpus: 1938 pages\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Corpus: 1938 pages\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Chunking pages (token-based)...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Chunking pages (token-based)...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Total chunks: 1937\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Total chunks: 1937\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Encoding chunk embeddings (dense)...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Encoding chunk embeddings (dense)...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0811b92461404aa2996b7bc005bb3412"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 0..255 done (elapsed 1.7s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 0..255 done (elapsed 1.7s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5fec119347043eaabbb742034dab852"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 256..511 done (elapsed 2.8s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 256..511 done (elapsed 2.8s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4bfe60a7bd043faa943f96e8e34ca1a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 512..767 done (elapsed 6.0s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 512..767 done (elapsed 6.0s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "699f3c5184f04981bab049edc27f20e5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 768..1023 done (elapsed 7.1s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 768..1023 done (elapsed 7.1s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9626b32023674ead9855a60781091595"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 1024..1279 done (elapsed 8.2s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 1024..1279 done (elapsed 8.2s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64775d15647e4c31890b645fa3348acd"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 1280..1535 done (elapsed 9.9s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 1280..1535 done (elapsed 9.9s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04ac960b71f8484d840b7b21d5f73562"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 1536..1791 done (elapsed 11.0s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 1536..1791 done (elapsed 11.0s)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "105774fef12e42d49fdaf2d5941b6a98"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[INFO]   chunks 1792..1936 done (elapsed 11.6s)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:  chunks 1792..1936 done (elapsed 11.6s)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Embeddings ready: (1937, 1024) in 11.6s\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Embeddings ready: (1937, 1024) in 11.6s\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] FAISS: using CPU index\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:FAISS: using CPU index\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Building BM25 index over chunks...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Building BM25 index over chunks...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] BM25 saved to /kaggle/working/artifacts/bm25_index.pkl\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:BM25 saved to /kaggle/working/artifacts/bm25_index.pkl\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Index build completed.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:Index build completed.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] Index ready in 15.4s. Loading queries...\n[INFO] Queries loaded: 6977. Starting answering...\n[INFO] [batch_answer] Start answering 6977 queries...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:[batch_answer] Start answering 6977 queries...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] [search_batch] start: 128 queries\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:[search_batch] start: 128 queries\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] [search_batch] embedded 128 q in 0.61s\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:[search_batch] embedded 128 q in 0.61s\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[INFO] [search_batch] dense candidates in 0.03s\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:run:[search_batch] dense candidates in 0.03s\npre tokenize: 100%|██████████| 2/2 [00:00<00:00, 188.06it/s]\nYou're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n 33%|███▎      | 1/3 [00:12<00:24, 12.06s/it]\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_48/3097148681.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    811\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 812\u001B[0;31m     \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_48/3097148681.py\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    802\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    803\u001B[0m     \u001B[0mt1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 804\u001B[0;31m     \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mretr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_answer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdfq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    805\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"[INFO] Answering done in {time.time()-t1:.1f}s. Saving submit.csv ...\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflush\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    806\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_48/3097148681.py\u001B[0m in \u001B[0;36mbatch_answer\u001B[0;34m(self, dfq)\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0mq_ids\u001B[0m   \u001B[0;34m=\u001B[0m \u001B[0mpart\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"q_id\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m             \u001B[0mt_chunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 771\u001B[0;31m             \u001B[0mdocs_lists\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mqueries\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop_k\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mFINAL_TOP_K_DOCS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    772\u001B[0m             \u001B[0mdur\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mt_chunk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    773\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mq_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdocs\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdocs_lists\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_48/3097148681.py\u001B[0m in \u001B[0;36msearch_batch\u001B[0;34m(self, queries, top_k)\u001B[0m\n\u001B[1;32m    715\u001B[0m                 \u001B[0mcand_docs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprelim_docs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlocal_rerank_k\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    716\u001B[0m                 \u001B[0mcand_texts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_doc_text_for_ce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq_text\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdense_chunks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbm25_chunks\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcand_docs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 717\u001B[0;31m                 \u001B[0mce_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reranker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq_text\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcand_texts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_bs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    718\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mce_scores\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    719\u001B[0m                     \u001B[0mce_min\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mce_max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mce_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mce_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_48/3097148681.py\u001B[0m in \u001B[0;36mscore\u001B[0;34m(self, query, docs, batch_size)\u001B[0m\n\u001B[1;32m    450\u001B[0m         \u001B[0mpairs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdocs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m         \u001B[0;31m# FlagLLMReranker сам батает пары внутри, batch_size можно игнорировать\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 452\u001B[0;31m         \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpairs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# список float\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    453\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/FlagEmbedding/abc/inference/AbsReranker.py\u001B[0m in \u001B[0;36mcompute_score\u001B[0;34m(self, sentence_pairs, **kwargs)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence_pairs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_devices\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 218\u001B[0;31m             return self.compute_score_single_gpu(\n\u001B[0m\u001B[1;32m    219\u001B[0m                 \u001B[0msentence_pairs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_devices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mctx_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/FlagEmbedding/inference/reranker/decoder_only/base.py\u001B[0m in \u001B[0;36mcompute_score_single_gpu\u001B[0;34m(self, sentence_pairs, batch_size, query_max_length, max_length, prompt, normalize, use_dataloader, num_workers, device, **kwargs)\u001B[0m\n\u001B[1;32m    490\u001B[0m                 \u001B[0mbatch_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mbatch_inputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    491\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 492\u001B[0;31m                 \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mbatch_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    493\u001B[0m                 \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    494\u001B[0m                 \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlast_logit_pool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'attention_mask'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    941\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    942\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 943\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    944\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_requested_to_return_tuple\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mis_configured_to_return_tuple\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mis_top_level_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    945\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    553\u001B[0m         \u001B[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 554\u001B[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001B[0m\u001B[1;32m    555\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    941\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    942\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 943\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    944\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_requested_to_return_tuple\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mis_configured_to_return_tuple\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mis_top_level_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    945\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    440\u001B[0m                 \u001B[0mall_hidden_states\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 442\u001B[0;31m             layer_outputs = decoder_layer(\n\u001B[0m\u001B[1;32m    443\u001B[0m                 \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    444\u001B[0m                 \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcausal_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gradient_checkpointing_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    287\u001B[0m         \u001B[0;31m# Self Attention\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 288\u001B[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001B[0m\u001B[1;32m    289\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    290\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    243\u001B[0m             \u001B[0mattention_interface\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mALL_ATTENTION_FUNCTIONS\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_attn_implementation\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 245\u001B[0;31m         attn_output, attn_weights = attention_interface(\n\u001B[0m\u001B[1;32m    246\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m             \u001B[0mquery_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001B[0m in \u001B[0;36msdpa_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0mis_causal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mis_causal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001B[0m\u001B[1;32m     67\u001B[0m         \u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m         \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 53.12 MiB is free. Process 4014 has 15.83 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 83.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ],
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 53.12 MiB is free. Process 4014 has 15.83 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 83.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
