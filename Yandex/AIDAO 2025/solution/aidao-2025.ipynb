{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 13294016,
     "sourceType": "datasetVersion",
     "datasetId": 8425885,
     "isSourceIdPinned": false
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 1. Setup & Load =====================\n",
    "import os, gc, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import kagglehub\n",
    "from glob import glob\n",
    "\n",
    "# --- Download dataset (cached by kagglehub) ---\n",
    "path = kagglehub.dataset_download(\"denismuradyan/dataset-olympics-aidao-yandex-2025\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# --- Columns (50) per README; dataset has NO headers ---\n",
    "COLS = [\n",
    "    'block_id','frame_idx','E_mu_Z','E_mu_phys_est','E_mu_X','E_nu1_X','E_nu2_X','E_nu1_Z','E_nu2_Z',\n",
    "    'N_mu_X','M_mu_XX','M_mu_XZ','M_mu_X','N_mu_Z','M_mu_ZZ','M_mu_Z',\n",
    "    'N_nu1_X','M_nu1_XX','M_nu1_XZ','M_nu1_X','N_nu1_Z','M_nu1_ZZ','M_nu1_Z',\n",
    "    'N_nu2_X','M_nu2_XX','M_nu2_XZ','M_nu2_X','N_nu2_Z','M_nu2_ZZ','M_nu2_Z',\n",
    "    'nTot','bayesImVoltage','opticalPower',\n",
    "    'polarizerVoltages[0]','polarizerVoltages[1]','polarizerVoltages[2]','polarizerVoltages[3]',\n",
    "    'temp_1','biasVoltage_1','temp_2','biasVoltage_2',\n",
    "    'synErr','N_EC_rounds','maintenance_flag','estimator_name','f_EC','E_mu_Z_est','R','s','p'\n",
    "]\n",
    "\n",
    "# --- Robust CSV selection: pick file with the largest #columns (~50) ---\n",
    "def count_cols(p):\n",
    "    try:\n",
    "        return pd.read_csv(p, header=None, nrows=3).shape[1]\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "csv_candidates = sorted(glob(os.path.join(path, \"*.csv\")))\n",
    "if not csv_candidates:\n",
    "    raise FileNotFoundError(\"No CSV files found under dataset path.\")\n",
    "\n",
    "csv_scored = [(p, count_cols(p)) for p in csv_candidates]\n",
    "csv_scored = [x for x in csv_scored if x[1] >= 40]  # expect ~50\n",
    "input_csv = sorted(csv_scored, key=lambda x: -x[1])[0][0]\n",
    "print(\"Using CSV:\", os.path.basename(input_csv))\n",
    "\n",
    "# --- Read WITHOUT header; keep strings where needed (estimator_name) ---\n",
    "df = pd.read_csv(input_csv, header=None, names=COLS, low_memory=False)\n",
    "print(\"Raw shape:\", df.shape)\n",
    "\n",
    "# --- Sort by (block_id, frame_idx) and add causal order index ---\n",
    "df = df.sort_values(['block_id','frame_idx']).reset_index(drop=True)\n",
    "df['_ord_'] = np.arange(len(df))\n",
    "\n",
    "# --- Window to predict: exactly 2000 rows starting at {block_id=1489460492, frame_idx=99} ---\n",
    "START_BLOCK, START_FRAME = 1489460492, 99\n",
    "target_len = 2000\n",
    "\n",
    "start_pos = df.index[(df.block_id==START_BLOCK) & (df.frame_idx==START_FRAME)]\n",
    "assert len(start_pos)>0, \"Start {block_id, frame_idx} not found in data\"\n",
    "start_ord = int(start_pos.min())\n",
    "end_ord   = start_ord + target_len - 1\n",
    "assert end_ord < len(df), \"Not enough rows for the 2000-frame window\"\n",
    "\n",
    "target_idx = np.arange(start_ord, end_ord+1)\n",
    "print(\"Target window ord:\", start_ord, \"→\", end_ord, \" (len:\", len(target_idx), \")\")\n",
    "\n",
    "# consistency checks\n",
    "assert 'E_mu_Z' in df.columns and 'block_id' in df.columns and 'frame_idx' in df.columns"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:07:40.471909Z",
     "iopub.execute_input": "2025-10-11T21:07:40.472287Z",
     "iopub.status.idle": "2025-10-11T21:07:44.698652Z",
     "shell.execute_reply.started": "2025-10-11T21:07:40.472264Z",
     "shell.execute_reply": "2025-10-11T21:07:44.697799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Path to dataset files: /kaggle/input/dataset-olympics-aidao-yandex-2025\nUsing CSV: frames_errors.csv\nRaw shape: (328617, 50)\nTarget window ord: 223704 → 225703  (len: 2000 )\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 2. Feature Engineering =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_group_lags(df, group_cols, col, lags):\n",
    "    g = df.groupby(group_cols)[col]\n",
    "    for L in lags:\n",
    "        df[f'{col}_lag{L}'] = g.shift(L)\n",
    "    return df\n",
    "\n",
    "def add_group_ema(df, group_cols, col, alphas):\n",
    "    # names: <col>_ema{alpha}, e.g. E_mu_Z_ema0.33\n",
    "    g = df.groupby(group_cols)[col]\n",
    "    for a in alphas:\n",
    "        cname = f\"{col}_ema{a}\"\n",
    "        df[cname] = g.transform(lambda s: s.ewm(alpha=a, adjust=False).mean())\n",
    "    return df\n",
    "\n",
    "def add_group_rollstd(df, group_cols, col, windows):\n",
    "    for w in windows:\n",
    "        df[f'{col}_std{w}'] = (\n",
    "            df.groupby(group_cols)[col]\n",
    "              .rolling(w, min_periods=2).std()\n",
    "              .reset_index(level=group_cols, drop=True)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Base signals to feature-ize\n",
    "base_signals = [c for c in ['E_mu_Z','E_mu_phys_est','opticalPower'] if c in df.columns]\n",
    "\n",
    "# Lags + EMA\n",
    "for c in base_signals:\n",
    "    df = add_group_lags(df, ['block_id'], c, lags=[1,2,3,4,5,8,12,16,24,32])\n",
    "    df = add_group_ema(df, ['block_id'], c, alphas=[0.2, 0.33, 0.5])\n",
    "\n",
    "# Volatility (rolling std)\n",
    "df = add_group_rollstd(df, ['block_id'], 'E_mu_Z', windows=[8,16])\n",
    "\n",
    "# Simple deltas/medians\n",
    "df['E_mu_Z_diff1'] = df.groupby('block_id')['E_mu_Z'].diff(1)\n",
    "if 'opticalPower' in df.columns:\n",
    "    df['opticalPower_diff1'] = df.groupby('block_id')['opticalPower'].diff(1)\n",
    "else:\n",
    "    df['opticalPower_diff1'] = 0.0\n",
    "\n",
    "df['E_mu_Z_med9'] = df.groupby('block_id')['E_mu_Z'].transform(lambda s: s.rolling(9, min_periods=2).median())\n",
    "df['E_mu_Z_med_dev'] = (df['E_mu_Z'] - df['E_mu_Z_med9']).abs()\n",
    "\n",
    "# Rolling max/min (lagged)\n",
    "for w in [8,16]:\n",
    "    df[f'E_mu_Z_max{w}_lag1'] = df.groupby('block_id')['E_mu_Z'].transform(lambda s: s.rolling(w, min_periods=2).max()).shift(1)\n",
    "    df[f'E_mu_Z_min{w}_lag1'] = df.groupby('block_id')['E_mu_Z'].transform(lambda s: s.rolling(w, min_periods=2).min()).shift(1)\n",
    "\n",
    "# Prev flags\n",
    "df['maint_prev'] = df.groupby('block_id')['maintenance_flag'].shift(1) if 'maintenance_flag' in df.columns else 0\n",
    "df['synErr_prev'] = df.groupby('block_id')['synErr'].shift(1) if 'synErr' in df.columns else 0\n",
    "df['N_EC_prev']   = df.groupby('block_id')['N_EC_rounds'].shift(1) if 'N_EC_rounds' in df.columns else 0\n",
    "\n",
    "# Block-wise seasonality proxies\n",
    "denom = max(1, df['frame_idx'].max())\n",
    "df['frame_sin'] = np.sin(2*np.pi*df['frame_idx']/denom)\n",
    "df['frame_cos'] = np.cos(2*np.pi*df['frame_idx']/denom)\n",
    "\n",
    "# Category for block id\n",
    "df['block_id_str'] = df['block_id'].astype(str)\n",
    "\n",
    "# Clean NaNs/Infs for ML features\n",
    "for c in ['E_mu_Z_diff1','opticalPower_diff1','E_mu_Z_med9','E_mu_Z_med_dev',\n",
    "          'E_mu_Z_max8_lag1','E_mu_Z_min8_lag1','E_mu_Z_max16_lag1','E_mu_Z_min16_lag1',\n",
    "          'E_mu_Z_std8','E_mu_Z_std16']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "\n",
    "# Feature list (exclude outputs/targets/ids)\n",
    "exclude_cols = set(['R','s','p','f_EC','estimator_name','E_mu_Z_est','_ord_'])\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols and c not in ['block_id','frame_idx']]\n",
    "df[feature_cols] = df[feature_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(\"Feature count:\", len(feature_cols))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:07:52.784500Z",
     "iopub.execute_input": "2025-10-11T21:07:52.784794Z",
     "iopub.status.idle": "2025-10-11T21:07:56.898325Z",
     "shell.execute_reply.started": "2025-10-11T21:07:52.784774Z",
     "shell.execute_reply": "2025-10-11T21:07:56.897694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Feature count: 97\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 3. Quantile CatBoost for E_mu_Z =====================\n",
    "import os\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Train/cal splits: all rows strictly before target window for training;\n",
    "# keep a small tail as calibration set\n",
    "train_mask = df['_ord_'] < start_ord\n",
    "train_idx_all = df.index[train_mask]\n",
    "\n",
    "cal_tail = min(500, len(train_idx_all)//5 if len(train_idx_all)>1000 else max(100, len(train_idx_all)//3))\n",
    "cal_idx = train_idx_all[-cal_tail:] if len(train_idx_all) > cal_tail else train_idx_all\n",
    "fit_idx = train_idx_all.difference(cal_idx)\n",
    "\n",
    "# Build design matrices\n",
    "def make_X(idx):\n",
    "    X = df.loc[idx, feature_cols + ['block_id_str']].copy()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    X['block_id_str'] = X['block_id_str'].astype('category')\n",
    "    return X\n",
    "\n",
    "X_fit_df = make_X(fit_idx)\n",
    "X_cal_df = make_X(cal_idx)\n",
    "X_tst_df = make_X(target_idx)\n",
    "\n",
    "y_fit = df.loc[fit_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "y_cal = df.loc[cal_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "\n",
    "task_type = 'GPU' if os.path.exists('/usr/bin/nvidia-smi') else 'CPU'\n",
    "cat_idx = [X_fit_df.columns.get_loc('block_id_str')]\n",
    "\n",
    "train_pool = Pool(X_fit_df, y_fit, cat_features=cat_idx)\n",
    "cal_pool   = Pool(X_cal_df, y_cal, cat_features=cat_idx)\n",
    "tst_pool   = Pool(X_tst_df,            cat_features=cat_idx)\n",
    "\n",
    "q_list   = [0.50, 0.80, 0.90, 0.95]\n",
    "models   = {}\n",
    "qhat     = {}   # preds on target\n",
    "qhat_cal = {}   # preds on cal\n",
    "\n",
    "for q in q_list:\n",
    "    mdl = CatBoostRegressor(\n",
    "        loss_function=f'Quantile:alpha={q}',\n",
    "        iterations=3000,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_state=42,\n",
    "        task_type=task_type,\n",
    "        verbose=250,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    mdl.fit(train_pool, eval_set=cal_pool, use_best_model=True)\n",
    "    models[q]   = mdl\n",
    "    qhat_cal[q] = mdl.predict(cal_pool)\n",
    "    qhat[q]     = mdl.predict(tst_pool)\n",
    "\n",
    "print(\"Quantile models trained.\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:08:40.375489Z",
     "iopub.execute_input": "2025-10-11T21:08:40.376103Z",
     "iopub.status.idle": "2025-10-11T21:28:20.841585Z",
     "shell.execute_reply.started": "2025-10-11T21:08:40.376078Z",
     "shell.execute_reply": "2025-10-11T21:28:20.840701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "0:\tlearn: 0.0021181\ttest: 0.0010694\tbest: 0.0010694 (0)\ttotal: 185ms\tremaining: 9m 15s\n250:\tlearn: 0.0001266\ttest: 0.0000553\tbest: 0.0000553 (250)\ttotal: 24.5s\tremaining: 4m 27s\n500:\tlearn: 0.0000889\ttest: 0.0000326\tbest: 0.0000326 (500)\ttotal: 49s\tremaining: 4m 4s\n750:\tlearn: 0.0000769\ttest: 0.0000248\tbest: 0.0000248 (750)\ttotal: 1m 13s\tremaining: 3m 40s\n1000:\tlearn: 0.0000710\ttest: 0.0000217\tbest: 0.0000217 (1000)\ttotal: 1m 38s\tremaining: 3m 17s\n1250:\tlearn: 0.0000666\ttest: 0.0000195\tbest: 0.0000195 (1250)\ttotal: 2m 3s\tremaining: 2m 53s\n1500:\tlearn: 0.0000636\ttest: 0.0000185\tbest: 0.0000185 (1500)\ttotal: 2m 29s\tremaining: 2m 28s\n1750:\tlearn: 0.0000607\ttest: 0.0000176\tbest: 0.0000176 (1750)\ttotal: 2m 57s\tremaining: 2m 6s\n2000:\tlearn: 0.0000583\ttest: 0.0000170\tbest: 0.0000170 (2000)\ttotal: 3m 23s\tremaining: 1m 41s\n2250:\tlearn: 0.0000569\ttest: 0.0000166\tbest: 0.0000166 (2250)\ttotal: 3m 49s\tremaining: 1m 16s\n2500:\tlearn: 0.0000553\ttest: 0.0000163\tbest: 0.0000163 (2500)\ttotal: 4m 15s\tremaining: 51s\n2750:\tlearn: 0.0000543\ttest: 0.0000161\tbest: 0.0000161 (2750)\ttotal: 4m 41s\tremaining: 25.5s\n2999:\tlearn: 0.0000533\ttest: 0.0000155\tbest: 0.0000155 (2997)\ttotal: 5m 8s\tremaining: 0us\n\nbestTest = 1.545176213e-05\nbestIteration = 2997\n\nShrink model to first 2998 iterations.\n0:\tlearn: 0.0024089\ttest: 0.0009576\tbest: 0.0009576 (0)\ttotal: 106ms\tremaining: 5m 16s\n250:\tlearn: 0.0000919\ttest: 0.0000527\tbest: 0.0000527 (250)\ttotal: 25.1s\tremaining: 4m 35s\n500:\tlearn: 0.0000668\ttest: 0.0000443\tbest: 0.0000443 (500)\ttotal: 48.8s\tremaining: 4m 3s\n750:\tlearn: 0.0000548\ttest: 0.0000372\tbest: 0.0000372 (750)\ttotal: 1m 12s\tremaining: 3m 38s\n1000:\tlearn: 0.0000486\ttest: 0.0000333\tbest: 0.0000333 (999)\ttotal: 1m 37s\tremaining: 3m 13s\n1250:\tlearn: 0.0000441\ttest: 0.0000299\tbest: 0.0000298 (1249)\ttotal: 2m 1s\tremaining: 2m 49s\n1500:\tlearn: 0.0000412\ttest: 0.0000278\tbest: 0.0000278 (1500)\ttotal: 2m 25s\tremaining: 2m 25s\n1750:\tlearn: 0.0000387\ttest: 0.0000257\tbest: 0.0000257 (1749)\ttotal: 2m 49s\tremaining: 2m 1s\n2000:\tlearn: 0.0000372\ttest: 0.0000247\tbest: 0.0000247 (2000)\ttotal: 3m 14s\tremaining: 1m 37s\n2250:\tlearn: 0.0000359\ttest: 0.0000234\tbest: 0.0000234 (2250)\ttotal: 3m 40s\tremaining: 1m 13s\n2500:\tlearn: 0.0000346\ttest: 0.0000222\tbest: 0.0000222 (2500)\ttotal: 4m 5s\tremaining: 48.9s\n2750:\tlearn: 0.0000334\ttest: 0.0000208\tbest: 0.0000208 (2750)\ttotal: 4m 29s\tremaining: 24.4s\n2999:\tlearn: 0.0000327\ttest: 0.0000200\tbest: 0.0000200 (2982)\ttotal: 4m 54s\tremaining: 0us\n\nbestTest = 2.001202126e-05\nbestIteration = 2982\n\nShrink model to first 2983 iterations.\n0:\tlearn: 0.0021986\ttest: 0.0007092\tbest: 0.0007092 (0)\ttotal: 103ms\tremaining: 5m 8s\n250:\tlearn: 0.0000508\ttest: 0.0000251\tbest: 0.0000251 (250)\ttotal: 25.2s\tremaining: 4m 35s\n500:\tlearn: 0.0000367\ttest: 0.0000197\tbest: 0.0000197 (500)\ttotal: 49.2s\tremaining: 4m 5s\n750:\tlearn: 0.0000302\ttest: 0.0000163\tbest: 0.0000163 (750)\ttotal: 1m 13s\tremaining: 3m 39s\n1000:\tlearn: 0.0000270\ttest: 0.0000139\tbest: 0.0000139 (1000)\ttotal: 1m 36s\tremaining: 3m 13s\n1250:\tlearn: 0.0000251\ttest: 0.0000126\tbest: 0.0000126 (1250)\ttotal: 2m\tremaining: 2m 49s\n1500:\tlearn: 0.0000239\ttest: 0.0000117\tbest: 0.0000117 (1500)\ttotal: 2m 25s\tremaining: 2m 24s\n1750:\tlearn: 0.0000227\ttest: 0.0000108\tbest: 0.0000108 (1750)\ttotal: 2m 49s\tremaining: 2m\n2000:\tlearn: 0.0000221\ttest: 0.0000105\tbest: 0.0000105 (2000)\ttotal: 3m 13s\tremaining: 1m 36s\n2250:\tlearn: 0.0000216\ttest: 0.0000103\tbest: 0.0000103 (2250)\ttotal: 3m 37s\tremaining: 1m 12s\n2500:\tlearn: 0.0000209\ttest: 0.0000100\tbest: 0.0000100 (2499)\ttotal: 4m 2s\tremaining: 48.3s\n2750:\tlearn: 0.0000204\ttest: 0.0000099\tbest: 0.0000099 (2746)\ttotal: 4m 25s\tremaining: 24s\n2999:\tlearn: 0.0000201\ttest: 0.0000097\tbest: 0.0000097 (2997)\ttotal: 4m 49s\tremaining: 0us\n\nbestTest = 9.680336381e-06\nbestIteration = 2997\n\nShrink model to first 2998 iterations.\n0:\tlearn: 0.0019622\ttest: 0.0005361\tbest: 0.0005361 (0)\ttotal: 102ms\tremaining: 5m 5s\n250:\tlearn: 0.0000298\ttest: 0.0000180\tbest: 0.0000180 (250)\ttotal: 24.3s\tremaining: 4m 26s\n500:\tlearn: 0.0000197\ttest: 0.0000115\tbest: 0.0000115 (500)\ttotal: 48.6s\tremaining: 4m 2s\n750:\tlearn: 0.0000172\ttest: 0.0000097\tbest: 0.0000097 (750)\ttotal: 1m 12s\tremaining: 3m 38s\n1000:\tlearn: 0.0000156\ttest: 0.0000087\tbest: 0.0000087 (998)\ttotal: 1m 37s\tremaining: 3m 14s\n1250:\tlearn: 0.0000146\ttest: 0.0000080\tbest: 0.0000080 (1249)\ttotal: 2m 1s\tremaining: 2m 49s\n1500:\tlearn: 0.0000139\ttest: 0.0000075\tbest: 0.0000075 (1500)\ttotal: 2m 25s\tremaining: 2m 25s\n1750:\tlearn: 0.0000133\ttest: 0.0000073\tbest: 0.0000073 (1750)\ttotal: 2m 48s\tremaining: 2m\n2000:\tlearn: 0.0000129\ttest: 0.0000070\tbest: 0.0000070 (2000)\ttotal: 3m 11s\tremaining: 1m 35s\n2250:\tlearn: 0.0000125\ttest: 0.0000069\tbest: 0.0000069 (2249)\ttotal: 3m 34s\tremaining: 1m 11s\n2500:\tlearn: 0.0000121\ttest: 0.0000068\tbest: 0.0000068 (2500)\ttotal: 3m 56s\tremaining: 47.2s\n2750:\tlearn: 0.0000119\ttest: 0.0000067\tbest: 0.0000067 (2749)\ttotal: 4m 18s\tremaining: 23.4s\n2999:\tlearn: 0.0000117\ttest: 0.0000066\tbest: 0.0000066 (2999)\ttotal: 4m 41s\tremaining: 0us\n\nbestTest = 6.566448056e-06\nbestIteration = 2999\n\nQuantile models trained.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# # ===================== Cell 3. Quantile XGBoost for E_mu_Z =====================\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # --- 1. Train/cal split ---\n",
    "# train_mask = df['_ord_'] < start_ord\n",
    "# train_idx_all = df.index[train_mask]\n",
    "\n",
    "# cal_tail = min(500, len(train_idx_all)//5 if len(train_idx_all)>1000 else max(100, len(train_idx_all)//3))\n",
    "# cal_idx = train_idx_all[-cal_tail:] if len(train_idx_all) > cal_tail else train_idx_all\n",
    "# fit_idx = train_idx_all.difference(cal_idx)\n",
    "\n",
    "# # --- 2. Build design matrices ---\n",
    "# def make_X(idx):\n",
    "#     X = df.loc[idx, feature_cols + ['block_id_str']].copy()\n",
    "#     X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "#     # One-hot для категориальной переменной, т.к. XGBoost не принимает category напрямую\n",
    "#     X = pd.get_dummies(X, columns=['block_id_str'], drop_first=True)\n",
    "#     return X\n",
    "\n",
    "# X_fit_df = make_X(fit_idx)\n",
    "# X_cal_df = make_X(cal_idx)\n",
    "# X_tst_df = make_X(target_idx)\n",
    "\n",
    "# y_fit = df.loc[fit_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "# y_cal = df.loc[cal_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "\n",
    "# # --- 3. DMatrix для XGBoost ---\n",
    "# dtrain = xgb.DMatrix(X_fit_df, label=y_fit)\n",
    "# dcal   = xgb.DMatrix(X_cal_df, label=y_cal)\n",
    "# dtest  = xgb.DMatrix(X_tst_df)\n",
    "\n",
    "# # --- 4. Определяем quantile objective ---\n",
    "# def quantile_loss(alpha):\n",
    "#     \"\"\"Возвращает custom objective для quantile loss.\"\"\"\n",
    "#     def _quantile_obj(y_pred, dtrain):\n",
    "#         y_true = dtrain.get_label()\n",
    "#         error = y_true - y_pred\n",
    "#         grad = np.where(error < 0, -alpha, 1 - alpha)\n",
    "#         hess = np.ones_like(grad)  # постоянный гессиан\n",
    "#         return grad, hess\n",
    "#     return _quantile_obj\n",
    "\n",
    "# # --- 5. Метрика для валидации ---\n",
    "# def quantile_eval(alpha):\n",
    "#     def _eval(y_pred, dtrain):\n",
    "#         y_true = dtrain.get_label()\n",
    "#         error = y_true - y_pred\n",
    "#         q_loss = np.mean(np.maximum(alpha * error, (alpha - 1) * error))\n",
    "#         return f'quantile-{alpha:.2f}', q_loss\n",
    "#     return _eval\n",
    "\n",
    "# # --- 6. Параметры XGBoost ---\n",
    "# base_params = {\n",
    "#     'tree_method': 'gpu_hist',    # используем GPU\n",
    "#     'predictor': 'gpu_predictor',\n",
    "#     'max_depth': 6,\n",
    "#     'eta': 0.05,\n",
    "#     'lambda': 3.0,\n",
    "#     'subsample': 1.0,\n",
    "#     'colsample_bytree': 1.0,\n",
    "#     'seed': 42,\n",
    "#     'objective': 'reg:squarederror'  # заглушка (реальный objective кастомный)\n",
    "# }\n",
    "\n",
    "# num_boost_round = 3000\n",
    "# early_stopping_rounds = 200\n",
    "\n",
    "# # --- 7. Обучаем модель для каждого квантиля ---\n",
    "# q_list   = [0.50, 0.80, 0.90, 0.95]\n",
    "# models   = {}\n",
    "# qhat     = {}   # preds on target\n",
    "# qhat_cal = {}   # preds on cal\n",
    "\n",
    "# for q in q_list:\n",
    "#     print(f\"\\n===== Training XGBoost Quantile alpha={q} =====\")\n",
    "#     bst = xgb.train(\n",
    "#         base_params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=num_boost_round,\n",
    "#         evals=[(dcal, 'cal')],\n",
    "#         obj=quantile_loss(q),\n",
    "#         feval=quantile_eval(q),\n",
    "#         early_stopping_rounds=early_stopping_rounds,\n",
    "#         verbose_eval=250\n",
    "#     )\n",
    "#     models[q]   = bst\n",
    "#     qhat_cal[q] = bst.predict(dcal)\n",
    "#     qhat[q]     = bst.predict(dtest)\n",
    "\n",
    "# print(\"✅ Quantile XGBoost models trained on GPU.\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 4. Per-block SHIFT+SCALE calibration of quantiles =====================\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# If for any reason cal_idx is small, fallback to last few thousand rows before start\n",
    "if len(cal_idx) < 200:\n",
    "    bt_len = 3000\n",
    "    cal_idx = np.arange(max(0, start_ord - bt_len), start_ord)\n",
    "\n",
    "blocks_cal = df.loc[cal_idx, 'block_id'].astype(int).to_numpy()\n",
    "y_cal      = df.loc[cal_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "q50_cal    = np.asarray(qhat_cal[0.50], dtype=float)\n",
    "\n",
    "iso_by_block = {}\n",
    "scale_by_block = {}\n",
    "\n",
    "dfc = pd.DataFrame({'block_id': blocks_cal, 'y': y_cal, 'q50': q50_cal})\n",
    "min_pts = 60\n",
    "\n",
    "for bid, grp in dfc.groupby('block_id'):\n",
    "    if len(grp) < min_pts:\n",
    "        continue\n",
    "    # 1) isotonic: q50 -> y\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(grp['q50'].values, grp['y'].values)\n",
    "    iso_by_block[int(bid)] = iso\n",
    "    # 2) scale tails: IQR ratio\n",
    "    iqr_y = np.subtract(*np.percentile(grp['y'].values, [75, 25]))\n",
    "    iqr_q = np.subtract(*np.percentile(grp['q50'].values, [75, 25]))\n",
    "    scale = 1.0 if iqr_q<=1e-8 else float(np.clip(iqr_y/iqr_q, 0.7, 1.6))\n",
    "    scale_by_block[int(bid)] = scale\n",
    "\n",
    "g_scale = float(np.median(list(scale_by_block.values()))) if len(scale_by_block)>0 else 1.0\n",
    "\n",
    "# Apply to target: for each quantile q use: iso(q50) + scale*(q - q50)\n",
    "blocks_t = df.loc[target_idx, 'block_id'].astype(int).to_numpy()\n",
    "q50_t = np.asarray(qhat[0.50], dtype=float)\n",
    "\n",
    "Eq = {}\n",
    "for q in q_list:\n",
    "    base = np.asarray(qhat[q], dtype=float)\n",
    "    out  = np.empty_like(base)\n",
    "    for i, bid in enumerate(blocks_t):\n",
    "        iso = iso_by_block.get(int(bid), None)\n",
    "        sc  = scale_by_block.get(int(bid), g_scale)\n",
    "        mu  = float(iso.predict([q50_t[i]])[0]) if iso is not None else q50_t[i]\n",
    "        out[i] = mu + sc * (base[i] - q50_t[i])\n",
    "    Eq[q] = np.clip(out, 1e-6, 0.4999)\n",
    "\n",
    "print(\"Shift+scale calibration applied.\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:28:20.842841Z",
     "iopub.execute_input": "2025-10-11T21:28:20.843065Z",
     "iopub.status.idle": "2025-10-11T21:28:20.946729Z",
     "shell.execute_reply.started": "2025-10-11T21:28:20.843048Z",
     "shell.execute_reply": "2025-10-11T21:28:20.946061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Shift+scale calibration applied.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 5. Adaptive alpha & final E_mu_Z_est =====================\n",
    "# Build alpha (risk-averse on volatility/trouble flags)\n",
    "vol8_t  = df.loc[target_idx, 'E_mu_Z_std8'].fillna(0.0).to_numpy()\n",
    "vol16_t = df.loc[target_idx, 'E_mu_Z_std16'].fillna(0.0).to_numpy()\n",
    "trend_up= (df.loc[target_idx, 'E_mu_Z_diff1'].fillna(0.0).to_numpy() > 0).astype(float)\n",
    "flags   = ((df.loc[target_idx, 'maintenance_flag'].fillna(0)==1).astype(float)\n",
    "          + (df.loc[target_idx, 'synErr_prev'].fillna(0)>0).astype(float)\n",
    "          + (df.loc[target_idx, 'N_EC_prev'].fillna(0)>1).astype(float))\n",
    "\n",
    "alpha = np.clip(0.865\n",
    "                + 0.35*np.clip(vol8_t,  0, 0.05)\n",
    "                + 0.25*np.clip(vol16_t, 0, 0.05)\n",
    "                + 0.02*trend_up\n",
    "                + 0.01*np.clip(flags,0,2), 0.83, 0.93)\n",
    "\n",
    "# --- robust: convert possible Series with nonzero index to numpy arrays ---\n",
    "def _as_np(x):\n",
    "    if isinstance(x, pd.Series):\n",
    "        return x.to_numpy(dtype=float)\n",
    "    return np.asarray(x, dtype=float)\n",
    "\n",
    "# Fetch calibrated quantiles safely (keys can be 0.5 or 0.50 etc.)\n",
    "q50 = _as_np(Eq.get(0.5,  Eq.get(0.50)))\n",
    "q80 = _as_np(Eq.get(0.8,  Eq.get(0.80)))\n",
    "q90 = _as_np(Eq.get(0.9,  Eq.get(0.90)))\n",
    "q95 = _as_np(Eq.get(0.95, Eq.get(0.95)))\n",
    "\n",
    "a = np.asarray(alpha, dtype=float)\n",
    "\n",
    "# --- vectorized piecewise-linear interpolation over quantiles ---\n",
    "E_pred_final = np.empty_like(q50, dtype=float)\n",
    "\n",
    "m1 = (a <= 0.50)\n",
    "m2 = (a > 0.50) & (a <= 0.80)\n",
    "m3 = (a > 0.80) & (a <= 0.90)\n",
    "m4 = (a > 0.90) & (a <= 0.95)\n",
    "m5 = (a > 0.95)\n",
    "\n",
    "E_pred_final[m1] = q50[m1]\n",
    "\n",
    "t = (a[m2] - 0.50) / 0.30\n",
    "E_pred_final[m2] = q50[m2] + t * (q80[m2] - q50[m2])\n",
    "\n",
    "t = (a[m3] - 0.80) / 0.10\n",
    "E_pred_final[m3] = q80[m3] + t * (q90[m3] - q80[m3])\n",
    "\n",
    "t = (a[m4] - 0.90) / 0.05\n",
    "E_pred_final[m4] = q90[m4] + t * (q95[m4] - q90[m4])\n",
    "\n",
    "E_pred_final[m5] = q95[m5]\n",
    "\n",
    "E_pred_final = np.clip(E_pred_final, 1e-6, 0.4999)\n",
    "print(\"E_pred_final stats:\", np.round([E_pred_final.min(), E_pred_final.mean(), E_pred_final.max()], 5))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:32:15.638864Z",
     "iopub.execute_input": "2025-10-11T21:32:15.639160Z",
     "iopub.status.idle": "2025-10-11T21:32:15.657906Z",
     "shell.execute_reply.started": "2025-10-11T21:32:15.639142Z",
     "shell.execute_reply": "2025-10-11T21:32:15.657205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "E_pred_final stats: [0.0142  0.0197  0.03649]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 6. Backtest headroom tuning (fixed to_numpy) =====================\n",
    "def H2(x):\n",
    "    x = np.clip(x, 1e-12, 1-1e-12)\n",
    "    return -(x*np.log2(x) + (1-x)*np.log2(1-x))\n",
    "\n",
    "# Backtest window before start\n",
    "bt_len = 12000\n",
    "bt_start = max(0, start_ord - bt_len)\n",
    "bt_idx = np.arange(bt_start, start_ord)\n",
    "if len(bt_idx) < 2000:\n",
    "    bt_idx = np.arange(max(0, start_ord-4000), start_ord)\n",
    "\n",
    "# Historical fEC median for safety baseline\n",
    "hist_mask = df['_ord_'] < start_ord\n",
    "fec_hist2 = (1.0 - df.loc[hist_mask, 'R'].astype(float)) / H2(df.loc[hist_mask, 'E_mu_Z'].astype(float))\n",
    "fec_hist2 = fec_hist2.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "fec_global_med = float(np.clip(np.nanmedian(fec_hist2), 1.06, 1.18))\n",
    "fec_block_med = (df.loc[hist_mask, ['block_id']]\n",
    "                   .assign(fec=fec_hist2)\n",
    "                   .groupby('block_id')['fec'].median()).clip(1.06, 1.20)\n",
    "# удобно как питоновский dict\n",
    "fec_block_med_map = {int(k): float(v) for k, v in fec_block_med.items() if pd.notna(v)}\n",
    "\n",
    "R_GRID = np.round(np.arange(0.50, 0.901, 0.05), 2)\n",
    "\n",
    "# --- strictly as numpy arrays ---\n",
    "vol8_b   = df.loc[bt_idx, 'E_mu_Z_std8'].fillna(0.0).to_numpy()\n",
    "vol16_b  = df.loc[bt_idx, 'E_mu_Z_std16'].fillna(0.0).to_numpy()\n",
    "trend_b  = (df.loc[bt_idx, 'E_mu_Z_diff1'].fillna(0.0).to_numpy() > 0).astype(float)\n",
    "\n",
    "maint_b  = (df.loc[bt_idx, 'maintenance_flag'].fillna(0).to_numpy()==1).astype(float)\n",
    "syn_b    = (df.loc[bt_idx, 'synErr_prev'].fillna(0).to_numpy()>0).astype(float)\n",
    "nec_b    = (df.loc[bt_idx, 'N_EC_prev'].fillna(0).to_numpy()>1).astype(float)\n",
    "flags_b  = maint_b + syn_b + nec_b     # numpy array!\n",
    "\n",
    "E_true_b = df.loc[bt_idx, 'E_mu_Z'].astype(float).to_numpy()\n",
    "blocks_b = df.loc[bt_idx, 'block_id'].astype(int).to_numpy()\n",
    "\n",
    "def eval_policy(params):\n",
    "    base_head, kv8, kv16, ktrend, s_ceiling = params\n",
    "    R_prev = None\n",
    "    R_seq, s_seq, Rt_true_seq = [], [], []\n",
    "\n",
    "    for j in range(len(bt_idx)):\n",
    "        block = int(blocks_b[j])\n",
    "\n",
    "        # per-block fec with global fallback\n",
    "        fec_row = 0.30*float(fec_block_med_map.get(block, fec_global_med)) + 0.70*fec_global_med\n",
    "        fec_eff = fec_row + 0.06*np.clip(vol8_b[j],0,0.05) + 0.04*np.clip(vol16_b[j],0,0.05)\n",
    "        if flags_b[j] >= 1:\n",
    "            fec_eff += 0.01\n",
    "        fec_eff = float(np.clip(fec_eff, 1.05, 1.20))\n",
    "\n",
    "        Rt_true = float(np.clip(1.0 - fec_eff*H2(E_true_b[j]), 0.0, 0.95))\n",
    "        Rt_true_seq.append(Rt_true)\n",
    "\n",
    "        hr = float(np.clip(base_head + kv8*vol8_b[j] + kv16*vol16_b[j] + ktrend*(trend_b[j]>0), 0.005, 0.030))\n",
    "        safe = max(0.0, Rt_true - hr)\n",
    "\n",
    "        r_cands = R_GRID[R_GRID <= safe]\n",
    "        r = float(r_cands.max()) if len(r_cands) else 0.50\n",
    "\n",
    "        allow_up2 = (vol8_b[j] < 0.007) and (vol16_b[j] < 0.01) and (trend_b[j] == 0) and (flags_b[j] == 0)\n",
    "        if R_prev is not None:\n",
    "            j_prev = int(np.where(R_GRID==np.round(R_prev,2))[0][0])\n",
    "            j_cur  = int(np.where(R_GRID==np.round(r,2))[0][0])\n",
    "            j_cur  = max(min(j_cur, j_prev+(2 if allow_up2 else 1)), j_prev-1)\n",
    "            r      = float(R_GRID[j_cur])\n",
    "\n",
    "        s = int(np.round(32000*r - 27200*Rt_true))\n",
    "        s = int(np.clip(s, 0, 4800))\n",
    "        if s >= s_ceiling:\n",
    "            pos = int(np.where(R_GRID==np.round(r,2))[0][0])\n",
    "            if pos>0:\n",
    "                r2 = float(R_GRID[pos-1])\n",
    "                s2 = int(np.round(32000*r2 - 27200*Rt_true))\n",
    "                s2 = int(np.clip(s2, 0, 4800))\n",
    "                if s2 <= (s_ceiling-80):\n",
    "                    r, s = r2, s2\n",
    "\n",
    "        R_seq.append(r); s_seq.append(s); R_prev = r\n",
    "\n",
    "    R_seq      = np.asarray(R_seq, dtype=float)\n",
    "    s_seq      = np.asarray(s_seq, dtype=int)\n",
    "    Rt_true_seq= np.asarray(Rt_true_seq, dtype=float)\n",
    "\n",
    "    fail_sur = ((R_seq > (Rt_true_seq + 0.003)) | (s_seq > (s_ceiling + 50))).mean()\n",
    "    score = R_seq.mean() - 6.0*fail_sur - 0.08*(s_seq/4800.0).mean()\n",
    "    return score, R_seq.mean(), fail_sur\n",
    "\n",
    "grid = [\n",
    "    (0.010, 0.30, 0.20, 0.004, 4700),\n",
    "    (0.010, 0.35, 0.25, 0.004, 4700),\n",
    "    (0.012, 0.35, 0.25, 0.004, 4700),\n",
    "    (0.010, 0.40, 0.25, 0.005, 4680),\n",
    "    (0.012, 0.40, 0.30, 0.005, 4680),\n",
    "    (0.008,  0.30, 0.20, 0.003, 4700),\n",
    "    (0.009,  0.35, 0.25, 0.003, 4720),\n",
    "    (0.007,  0.28, 0.18, 0.003, 4720),\n",
    "    (0.007,  0.25, 0.15, 0.003, 4740),\n",
    "    (0.006,  0.25, 0.15, 0.002, 4740),\n",
    "]\n",
    "best = None\n",
    "for g in grid:\n",
    "    sc, rmean, failr = eval_policy(g)\n",
    "    print(f\"grid{g}: score={sc:.5f}  R̄={rmean:.3f}  fail~{failr:.3f}\")\n",
    "    if (best is None) or (sc > best[0]):\n",
    "        best = (sc, g)\n",
    "print(\"Best headroom grid:\", best)\n",
    "best_params = best[1]"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:34:02.916239Z",
     "iopub.execute_input": "2025-10-11T21:34:02.916823Z",
     "iopub.status.idle": "2025-10-11T21:34:11.263327Z",
     "shell.execute_reply.started": "2025-10-11T21:34:02.916802Z",
     "shell.execute_reply": "2025-10-11T21:34:11.262606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "grid(0.01, 0.3, 0.2, 0.004, 4700): score=0.74366  R̄=0.790  fail~0.000\ngrid(0.01, 0.35, 0.25, 0.004, 4700): score=0.74356  R̄=0.790  fail~0.000\ngrid(0.012, 0.35, 0.25, 0.004, 4700): score=0.74283  R̄=0.788  fail~0.000\ngrid(0.01, 0.4, 0.25, 0.005, 4680): score=0.74329  R̄=0.789  fail~0.000\ngrid(0.012, 0.4, 0.3, 0.005, 4680): score=0.74256  R̄=0.788  fail~0.000\ngrid(0.008, 0.3, 0.2, 0.003, 4700): score=0.74409  R̄=0.792  fail~0.000\ngrid(0.009, 0.35, 0.25, 0.003, 4720): score=0.74360  R̄=0.791  fail~0.000\ngrid(0.007, 0.28, 0.18, 0.003, 4720): score=0.74456  R̄=0.793  fail~0.000\ngrid(0.007, 0.25, 0.15, 0.003, 4740): score=0.74463  R̄=0.793  fail~0.000\ngrid(0.006, 0.25, 0.15, 0.002, 4740): score=0.74469  R̄=0.794  fail~0.000\nBest headroom grid: (0.7446869166666668, (0.006, 0.25, 0.15, 0.002, 4740))\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 7. Fail-Classifier (risk surrogate) — FIXED labels & split =====================\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1) Метка риска: (a) доп. раунды   (b) экстремально высокий synErr относительно блока ---\n",
    "hist_mask = df['_ord_'] < start_ord\n",
    "\n",
    "nec = df['N_EC_rounds'].fillna(1).astype(int)\n",
    "syn = df['synErr'].fillna(0).astype(float)\n",
    "\n",
    "# robust-порог по synErr внутри блока: med + 3*MAD (MAD=median(|x-med|), устойчива к выбросам)\n",
    "blk_med = syn[hist_mask].groupby(df.loc[hist_mask, 'block_id']).transform('median')\n",
    "blk_mad = (syn[hist_mask].groupby(df.loc[hist_mask, 'block_id'])\n",
    "                      .transform(lambda s: (s - s.median()).abs().median()).replace(0, np.nan))\n",
    "blk_mad = blk_mad.fillna(blk_mad.median())  # на случай констант в блоке\n",
    "\n",
    "syn_hist = syn.loc[hist_mask].to_numpy()\n",
    "thr = (blk_med + 3.0*blk_mad).to_numpy()\n",
    "syn_extreme_hist = (syn_hist > thr).astype(int)\n",
    "\n",
    "# отправим метку только для истории; на будущем участке она не используется\n",
    "y_hist = ((nec.loc[hist_mask] >= 2).astype(int).to_numpy() | syn_extreme_hist).astype(int)\n",
    "\n",
    "# --- 2) Признаки (онлайн-безопасные) ---\n",
    "cand_base = [\n",
    "    'E_mu_Z_lag1','E_mu_Z_lag2','E_mu_Z_lag3',\n",
    "    'E_mu_Z_ema0.33','E_mu_Z_ema0.5','E_mu_Z_std8','E_mu_Z_std16',\n",
    "    'opticalPower_ema0.33','opticalPower_ema0.5','opticalPower_diff1',\n",
    "    'maint_prev','synErr_prev','N_EC_prev',\n",
    "    'frame_sin','frame_cos',\n",
    "    'E_mu_phys_est_lag1','E_mu_phys_est_lag2','E_mu_phys_est_lag24',\n",
    "    'polarizerVoltages[2]','polarizerVoltages[3]'\n",
    "]\n",
    "clf_base_cols = [c for c in cand_base if c in df.columns]\n",
    "\n",
    "X_hist = df.loc[hist_mask, clf_base_cols].copy().fillna(0.0)\n",
    "\n",
    "# Контрольные признаки для маржи Шеннона (как раньше, только по истории)\n",
    "fec_const = 1.12\n",
    "def H2(x):\n",
    "    x = np.clip(x, 1e-12, 1-1e-12)\n",
    "    return -(x*np.log2(x) + (1-x)*np.log2(1-x))\n",
    "\n",
    "X_hist['E_proxy']  = df.loc[hist_mask, 'E_mu_Z'].astype(float).values\n",
    "X_hist['R_hist']   = df.loc[hist_mask, 'R'].astype(float).fillna(0.75).values\n",
    "X_hist['s_hist']   = df.loc[hist_mask, 's'].astype(float).fillna(2400).values\n",
    "X_hist['p_hist']   = df.loc[hist_mask, 'p'].astype(float).fillna(2400).values\n",
    "X_hist['shannon_margin'] = (1.0 - fec_const*H2(X_hist['E_proxy'].values)) - X_hist['R_hist'].values\n",
    "\n",
    "# --- 3) Стратифицированный train/val сплит по прошлым данным ---\n",
    "idx_hist = np.where(hist_mask)[0]\n",
    "# ограничим объём для ускорения, но сохраним баланс\n",
    "max_train_rows = 120_000\n",
    "if len(idx_hist) > max_train_rows:\n",
    "    # возьмём «хвост» истории, который ближе к таргету\n",
    "    idx_hist = idx_hist[-max_train_rows:]\n",
    "    X_hist   = X_hist.iloc[-max_train_rows:, :]\n",
    "    y_hist   = y_hist[-max_train_rows:]\n",
    "\n",
    "# Проверим, что в выборке есть обе классы; если нет — ослабим правило synErr\n",
    "if len(np.unique(y_hist)) < 2:\n",
    "    q90 = np.nanpercentile(syn_hist, 90)\n",
    "    y_hist = ((nec.loc[hist_mask] >= 2).astype(int).to_numpy() | (syn_hist > q90)).astype(int)\n",
    "\n",
    "# Теперь сплит\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_hist.values, y_hist, test_size=0.15, random_state=2025, stratify=y_hist\n",
    ")\n",
    "\n",
    "# Балансировка классов\n",
    "pos_w = max(1.0, (len(y_fit)-y_fit.sum())/max(1,y_fit.sum()))\n",
    "w_fit = np.where(y_fit==1, pos_w, 1.0).astype(np.float32)\n",
    "w_val = np.where(y_val==1, pos_w, 1.0).astype(np.float32)\n",
    "\n",
    "clf_features = list(X_hist.columns)\n",
    "train_pool = Pool(X_fit, y_fit, weight=w_fit, feature_names=clf_features)\n",
    "valid_pool = Pool(X_val, y_val, weight=w_val, feature_names=clf_features)\n",
    "\n",
    "# --- 4) Обучение классификатора ---\n",
    "clf = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=6.0,\n",
    "    random_strength=0.2,\n",
    "    iterations=1200,\n",
    "    early_stopping_rounds=200,\n",
    "    random_seed=2025,\n",
    "    task_type='CPU',\n",
    "    verbose=150\n",
    ")\n",
    "clf.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=150)\n",
    "print(\"Fail-classifier trained.\")\n",
    "\n",
    "# Быстрые sanity-метрики\n",
    "p_val = clf.predict_proba(valid_pool)[:,1]\n",
    "print(\"Val mean P(fail):\", float(p_val.mean()))\n",
    "print(\"Val P(fail) on positives / negatives:\",\n",
    "      float(p_val[y_val==1].mean()) if (y_val==1).any() else np.nan, \"/\",\n",
    "      float(p_val[y_val==0].mean()) if (y_val==0).any() else np.nan)\n",
    "\n",
    "# --- 5) Инференс-хелперы (как раньше) ---\n",
    "def build_clf_vector(row, E_proxy_val, R, S):\n",
    "    x = {c: float(row.get(c, 0.0) or 0.0) for c in clf_base_cols}\n",
    "    x['E_proxy']  = float(E_proxy_val)\n",
    "    x['R_hist']   = float(R)\n",
    "    x['s_hist']   = float(S)\n",
    "    x['p_hist']   = float(4800 - S)\n",
    "    x['shannon_margin'] = (1.0 - fec_const*H2(x['E_proxy'])) - x['R_hist']\n",
    "    return np.array([x[c] for c in clf_features], dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "def fail_prob_predict(E, R, s, row_features):\n",
    "    vec = build_clf_vector(row_features, E_proxy_val=E, R=R, S=s)\n",
    "    return float(clf.predict_proba(vec)[0,1])"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:38:09.205405Z",
     "iopub.execute_input": "2025-10-11T21:38:09.205799Z",
     "iopub.status.idle": "2025-10-11T21:38:32.617540Z",
     "shell.execute_reply.started": "2025-10-11T21:38:09.205777Z",
     "shell.execute_reply": "2025-10-11T21:38:32.616664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "0:\tlearn: 0.6420794\ttest: 0.6427351\tbest: 0.6427351 (0)\ttotal: 18.2ms\tremaining: 21.8s\n150:\tlearn: 0.2165208\ttest: 0.2206881\tbest: 0.2206881 (150)\ttotal: 2.71s\tremaining: 18.8s\n300:\tlearn: 0.1951628\ttest: 0.2025633\tbest: 0.2025633 (300)\ttotal: 5.51s\tremaining: 16.4s\n450:\tlearn: 0.1819543\ttest: 0.1926255\tbest: 0.1926255 (450)\ttotal: 8.22s\tremaining: 13.7s\n600:\tlearn: 0.1720487\ttest: 0.1854918\tbest: 0.1854918 (600)\ttotal: 10.9s\tremaining: 10.9s\n750:\tlearn: 0.1637275\ttest: 0.1801858\tbest: 0.1801858 (750)\ttotal: 13.7s\tremaining: 8.18s\n900:\tlearn: 0.1562915\ttest: 0.1754379\tbest: 0.1754379 (900)\ttotal: 16.4s\tremaining: 5.44s\n1050:\tlearn: 0.1502793\ttest: 0.1719625\tbest: 0.1719584 (1049)\ttotal: 19.1s\tremaining: 2.71s\n1199:\tlearn: 0.1451740\ttest: 0.1695967\tbest: 0.1695967 (1199)\ttotal: 21.8s\tremaining: 0us\n\nbestTest = 0.1695966996\nbestIteration = 1199\n\nFail-classifier trained.\nVal mean P(fail): 0.7101893536742658\nVal P(fail) on positives / negatives: 0.9192122422636733 / 0.19802918790114005\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "print(\"Val AUC:\", roc_auc_score(y_val, p_val))\n",
    "print(\"Val AP :\", average_precision_score(y_val, p_val))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:39:34.610376Z",
     "iopub.execute_input": "2025-10-11T21:39:34.610952Z",
     "iopub.status.idle": "2025-10-11T21:39:34.627830Z",
     "shell.execute_reply.started": "2025-10-11T21:39:34.610929Z",
     "shell.execute_reply": "2025-10-11T21:39:34.627069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Val AUC: 0.979629177030646\nVal AP : 0.9920385684845808\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 8. f_EC regression (on successful frames) =====================\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Successful frames: synErr==0 & N_EC_rounds==1\n",
    "train_mask = df['_ord_'] < start_ord\n",
    "good_mask  = train_mask & (df['synErr'].fillna(0)==0) & (df['N_EC_rounds'].fillna(1)==1)\n",
    "\n",
    "def H2(x):\n",
    "    x = np.clip(x, 1e-12, 1-1e-12)\n",
    "    return -(x*np.log2(x) + (1-x)*np.log2(1-x))\n",
    "\n",
    "fec_target = ((1.0 - df.loc[good_mask, 'R'].astype(float)) /\n",
    "              H2(df.loc[good_mask, 'E_mu_Z'].astype(float)))\n",
    "fec_target = fec_target.replace([np.inf,-np.inf], np.nan).clip(1.04, 1.25)\n",
    "\n",
    "fec_feats = [\n",
    "    c for c in [\n",
    "        'E_mu_Z_lag1','E_mu_Z_lag2','E_mu_Z_lag3',\n",
    "        'E_mu_Z_ema0.2','E_mu_Z_ema0.33','E_mu_Z_ema0.5',\n",
    "        'E_mu_Z_std8','E_mu_Z_std16',\n",
    "        'opticalPower_ema0.33','opticalPower_ema0.5','opticalPower_diff1',\n",
    "        'frame_sin','frame_cos','maint_prev','synErr_prev','N_EC_prev',\n",
    "        'E_mu_phys_est_lag1','E_mu_phys_est_lag2',\n",
    "        'polarizerVoltages[2]','polarizerVoltages[3]',\n",
    "        'block_id_str'\n",
    "    ] if c in df.columns or c=='block_id_str'\n",
    "]\n",
    "fec_feats = list(dict.fromkeys(fec_feats))\n",
    "\n",
    "X_fec = df.loc[good_mask, fec_feats].copy()\n",
    "X_fec['block_id_str'] = X_fec['block_id_str'].astype('category')\n",
    "y_fec = fec_target.astype(float).to_numpy()\n",
    "\n",
    "cat_idx = [X_fec.columns.get_loc('block_id_str')]\n",
    "fec_model = CatBoostRegressor(\n",
    "    loss_function='MAE',\n",
    "    depth=6, iterations=1200, learning_rate=0.05, l2_leaf_reg=3.0,\n",
    "    random_state=2025, task_type='CPU', verbose=200, early_stopping_rounds=100\n",
    ")\n",
    "fec_model.fit(Pool(X_fec, y_fec, cat_features=cat_idx),\n",
    "              eval_set=Pool(X_fec, y_fec, cat_features=cat_idx),\n",
    "              use_best_model=True)\n",
    "\n",
    "# Predict fEC on target\n",
    "X_fec_t = df.loc[target_idx, fec_feats].copy()\n",
    "X_fec_t['block_id_str'] = X_fec_t['block_id_str'].astype('category')\n",
    "fec_hat = fec_model.predict(Pool(X_fec_t, cat_features=[X_fec_t.columns.get_loc('block_id_str')]))\n",
    "fec_hat = np.clip(fec_hat, 1.05, 1.22)\n",
    "print(\"f_EC regression ready. Stats:\", float(np.mean(fec_hat)), float(np.min(fec_hat)), float(np.max(fec_hat)))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T21:39:57.456879Z",
     "iopub.execute_input": "2025-10-11T21:39:57.457160Z",
     "iopub.status.idle": "2025-10-11T21:39:58.628713Z",
     "shell.execute_reply.started": "2025-10-11T21:39:57.457142Z",
     "shell.execute_reply": "2025-10-11T21:39:58.627845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "0:\tlearn: 0.0397414\ttest: 0.0397414\tbest: 0.0397414 (0)\ttotal: 991us\tremaining: 1.19s\n200:\tlearn: 0.0003470\ttest: 0.0007000\tbest: 0.0007000 (200)\ttotal: 186ms\tremaining: 924ms\n400:\tlearn: 0.0000175\ttest: 0.0004039\tbest: 0.0004038 (398)\ttotal: 399ms\tremaining: 795ms\n600:\tlearn: 0.0000012\ttest: 0.0003906\tbest: 0.0003906 (599)\ttotal: 609ms\tremaining: 607ms\n800:\tlearn: 0.0000005\ttest: 0.0003902\tbest: 0.0003902 (796)\ttotal: 856ms\tremaining: 426ms\n1000:\tlearn: 0.0000004\ttest: 0.0003901\tbest: 0.0003901 (923)\ttotal: 952ms\tremaining: 189ms\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.0003900910056\nbestIteration = 923\n\nShrink model to first 924 iterations.\nf_EC regression ready. Stats: 1.170554499999395 1.0943716332725422 1.22\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 9. Lambda tuning for risk-aware utility — FAST + PROGRESS =====================\n",
    "import time\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "R_GRID = np.round(np.arange(0.50, 0.901, 0.05), 2)\n",
    "\n",
    "# --- Параметры быстродействия ---\n",
    "BT_LEN_LAM     = 2000          # длина хвоста для подбора λ (можешь поднять до 3000/4000)\n",
    "STEP_LOG       = 300           # как часто печатать прогресс (кадров)\n",
    "S_FAST_DS      = [0, +120, -120]  # смещения s вокруг s*\n",
    "MAX_R_SPAN     = 2             # использовать до 2 значений R вокруг базового (вместо 3)\n",
    "\n",
    "# --- Хвост для λ-подбора ---\n",
    "bt_idx_lam = bt_idx[-BT_LEN_LAM:] if len(bt_idx) > BT_LEN_LAM else bt_idx\n",
    "n_lam = len(bt_idx_lam)\n",
    "\n",
    "# --- Все массивы строго как numpy ---\n",
    "blocks_b = df.loc[bt_idx_lam, 'block_id'].astype(int).to_numpy()\n",
    "vol8_b   = df.loc[bt_idx_lam, 'E_mu_Z_std8'].fillna(0.0).to_numpy()\n",
    "vol16_b  = df.loc[bt_idx_lam, 'E_mu_Z_std16'].fillna(0.0).to_numpy()\n",
    "trend_b  = (df.loc[bt_idx_lam, 'E_mu_Z_diff1'].fillna(0.0).to_numpy() > 0).astype(float)\n",
    "\n",
    "maint_b  = (df.loc[bt_idx_lam, 'maintenance_flag'].fillna(0).to_numpy()==1).astype(float)\n",
    "syn_b    = (df.loc[bt_idx_lam, 'synErr_prev'].fillna(0).to_numpy()>0).astype(float)\n",
    "nec_b    = (df.loc[bt_idx_lam, 'N_EC_prev'].fillna(0).to_numpy()>1).astype(float)\n",
    "flags_b  = maint_b + syn_b + nec_b\n",
    "\n",
    "E_true_b = df.loc[bt_idx_lam, 'E_mu_Z'].astype(float).to_numpy()\n",
    "\n",
    "# --- fEC на окне подборa λ ---\n",
    "X_fec_b = df.loc[bt_idx_lam, fec_feats].copy()\n",
    "X_fec_b['block_id_str'] = X_fec_b['block_id_str'].astype('category')\n",
    "fec_hat_b = fec_model.predict(Pool(X_fec_b, cat_features=[X_fec_b.columns.get_loc('block_id_str')]))\n",
    "fec_hat_b = np.clip(fec_hat_b, 1.05, 1.22)\n",
    "\n",
    "# --- Предвычислим Rt и hr (они не зависят от R/s) ---\n",
    "def H2(x):\n",
    "    x = np.clip(x, 1e-12, 1-1e-12)\n",
    "    return -(x*np.log2(x) + (1-x)*np.log2(1-x))\n",
    "\n",
    "Rt_b = np.clip(1.0 - fec_hat_b * H2(E_true_b), 0.0, 0.95)\n",
    "base_head, kv8, kv16, ktrend, s_ceiling = best_params\n",
    "hr_b  = np.clip(base_head + kv8*vol8_b + kv16*vol16_b + ktrend*(trend_b>0), 0.004, 0.030)\n",
    "safe_b= np.maximum(0.0, Rt_b - hr_b)\n",
    "\n",
    "# --- Базовые фичи классификатора для всех кадров (одним куском) ---\n",
    "base_cols = [c for c in clf_base_cols if c in clf_features]\n",
    "base_mat  = df.loc[bt_idx_lam, base_cols].fillna(0.0).to_numpy(dtype=float)\n",
    "\n",
    "# --- Позиции служебных признаков в матрице классификатора ---\n",
    "pos_map = {name: clf_features.index(name) for name in ['E_proxy','R_hist','s_hist','p_hist','shannon_margin']}\n",
    "base_pos = np.array([clf_features.index(c) for c in base_cols], dtype=int)\n",
    "n_feat   = len(clf_features)\n",
    "\n",
    "fec_const = 1.12\n",
    "\n",
    "def choose_R_list(j, R_prev):\n",
    "    # базовый r0 из safe_b[j]\n",
    "    r_cands = R_GRID[R_GRID <= safe_b[j]]\n",
    "    r0 = float(r_cands.max()) if len(r_cands) else 0.50\n",
    "\n",
    "    allow_up2 = (vol8_b[j] < 0.007) and (vol16_b[j] < 0.01) and (trend_b[j] == 0) and (flags_b[j] == 0)\n",
    "    if R_prev is not None:\n",
    "        j_prev = int(np.where(R_GRID==np.round(R_prev,2))[0][0])\n",
    "        j_cur  = int(np.where(R_GRID==np.round(r0,2))[0][0])\n",
    "        j_cur  = max(min(j_cur, j_prev+(2 if allow_up2 else 1)), j_prev-1)\n",
    "        r0     = float(R_GRID[j_cur])\n",
    "\n",
    "    j0   = int(np.where(R_GRID==np.round(r0,2))[0][0])\n",
    "    j_lo = max(0, j0-1)\n",
    "    j_hi = min(len(R_GRID)-1, j0+1)\n",
    "    # Ограничим до MAX_R_SPAN значений (ускорение)\n",
    "    js = list(range(j_lo, j_hi+1))[:MAX_R_SPAN]\n",
    "    return [float(R_GRID[k]) for k in js]\n",
    "\n",
    "def backtest_weights(lmbd):\n",
    "    lam1, lam2, lam3, lam4 = lmbd\n",
    "    R_prev = None\n",
    "    util_sum = r_sum = fail_true_sum = 0.0\n",
    "\n",
    "    # Предвыделение массивов под кандидатов (максимум 3*|S_FAST_DS| ~ 9)\n",
    "    maxC = MAX_R_SPAN * len(S_FAST_DS)\n",
    "    Xcand = np.zeros((maxC, n_feat), dtype=np.float32)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for j in range(n_lam):\n",
    "        # R-кандидаты\n",
    "        R_list = choose_R_list(j, R_prev)\n",
    "        # Список кандидатов (R, s, p, Rt)\n",
    "        C = []\n",
    "        Rt = Rt_b[j]\n",
    "        for Rv in R_list:\n",
    "            s_star = int(np.clip(round(32000*Rv - 27200*Rt), 0, 4800))\n",
    "            for ds in S_FAST_DS:\n",
    "                s = int(np.clip(s_star + ds, 0, 4800))\n",
    "                C.append((Rv, s, 4800 - s, Rt))\n",
    "        # дедуп\n",
    "        C = list({(R,s,p):Rt for (R,s,p,Rt) in C}.items())\n",
    "        C = [(R, s, p, Rt) for (R,s,p), Rt in C]\n",
    "        Cn = len(C)\n",
    "\n",
    "        # заполнение батч-матрицы\n",
    "        Xcand.fill(0.0)\n",
    "        Xcand[:Cn, base_pos] = base_mat[j][None, :]\n",
    "\n",
    "        CR  = np.fromiter((c[0] for c in C), dtype=float, count=Cn)\n",
    "        CS  = np.fromiter((c[1] for c in C), dtype=float, count=Cn)\n",
    "        CP  = 4800.0 - CS\n",
    "        CRt = np.fromiter((c[3] for c in C), dtype=float, count=Cn)\n",
    "\n",
    "        e = float(E_true_b[j])\n",
    "        sm = (1.0 - fec_const*H2(e)) - CR\n",
    "        dR = 0.0 if R_prev is None else np.abs(CR - R_prev)\n",
    "\n",
    "        Xcand[:Cn, pos_map['E_proxy']]         = e\n",
    "        Xcand[:Cn, pos_map['R_hist']]          = CR\n",
    "        Xcand[:Cn, pos_map['s_hist']]          = CS\n",
    "        Xcand[:Cn, pos_map['p_hist']]          = CP\n",
    "        Xcand[:Cn, pos_map['shannon_margin']]  = sm\n",
    "\n",
    "        # риск фейла для всех кандидатов сразу\n",
    "        pf = clf.predict_proba(Xcand[:Cn, :])[:,1]\n",
    "\n",
    "        # utility\n",
    "        R_eff  = (32000.0*CR - CS) / 27200.0\n",
    "        margin = R_eff - CRt\n",
    "        util   = (CR\n",
    "                  - lam1*pf\n",
    "                  - lam2*np.maximum(0.0, margin)\n",
    "                  - lam3*(CS/4800.0)\n",
    "                  - lam4*dR)\n",
    "\n",
    "        k = int(np.argmax(util))\n",
    "        best_R, best_S = float(CR[k]), int(CS[k])\n",
    "\n",
    "        # потолок s — понижаем R на ступень, если выгодно\n",
    "        if best_S >= s_ceiling:\n",
    "            pos = int(np.where(R_GRID==np.round(best_R,2))[0][0])\n",
    "            if pos > 0:\n",
    "                R2 = float(R_GRID[pos-1])\n",
    "                s2 = int(np.clip(round(32000*R2 - 27200*Rt), 0, 4800))\n",
    "                if s2 <= (s_ceiling - 60):\n",
    "                    best_R, best_S = R2, s2\n",
    "\n",
    "        util_sum     += float(util[k])\n",
    "        r_sum        += best_R\n",
    "        fail_true_sum+= float(df.at[bt_idx_lam[j], 'N_EC_rounds'] >= 2)\n",
    "        R_prev = best_R\n",
    "\n",
    "        # прогресс\n",
    "        if (j+1) % STEP_LOG == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"  .. {j+1}/{n_lam} frames for λ={lmbd}, \"\n",
    "                  f\"avg util={util_sum/(j+1):.5f}, R̄={r_sum/(j+1):.3f}, \"\n",
    "                  f\"fail≈{fail_true_sum/(j+1):.3f}, {elapsed:.1f}s\", flush=True)\n",
    "\n",
    "    return (util_sum/n_lam, r_sum/n_lam, fail_true_sum/n_lam)\n",
    "\n",
    "# --- сетка λ ---\n",
    "lambda_grid = [\n",
    "    (2.0, 1.0, 0.05, 0.02),\n",
    "    (2.5, 1.2, 0.05, 0.02),\n",
    "    (3.0, 1.2, 0.06, 0.03),\n",
    "    (3.5, 1.4, 0.06, 0.03),\n",
    "    (4.0, 1.6, 0.07, 0.04),\n",
    "    (4.5, 1.6, 0.07, 0.04),\n",
    "    (5.0, 1.8, 0.07, 0.05),\n",
    "]\n",
    "\n",
    "best_tuple = None\n",
    "for lambdas in lambda_grid:\n",
    "    print(f\"λ={lambdas} → running backtest on {n_lam} frames ...\", flush=True)\n",
    "    u, rbar, failr = backtest_weights(lambdas)\n",
    "    print(f\"λ={lambdas}: util={u:.5f}  R̄={rbar:.3f}  fail≈{failr:.3f}\", flush=True)\n",
    "    if (best_tuple is None) or (u > best_tuple[0]):\n",
    "        best_tuple = (u, lambdas)\n",
    "\n",
    "print(\"Best λ:\", best_tuple[1])\n",
    "LAM = best_tuple[1]"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T22:00:21.346781Z",
     "iopub.execute_input": "2025-10-11T22:00:21.347267Z",
     "iopub.status.idle": "2025-10-11T22:00:39.436436Z",
     "shell.execute_reply.started": "2025-10-11T22:00:21.347247Z",
     "shell.execute_reply": "2025-10-11T22:00:39.435786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "λ=(2.0, 1.0, 0.05, 0.02) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.07768, R̄=0.771, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.06943, R̄=0.766, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.04923, R̄=0.768, fail≈0.677, 1.1s\n  .. 1200/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.05658, R̄=0.769, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.08445, R̄=0.768, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(2.0, 1.0, 0.05, 0.02), avg util=0.07849, R̄=0.770, fail≈0.688, 2.3s\nλ=(2.0, 1.0, 0.05, 0.02): util=0.09440  R̄=0.769  fail≈0.683\nλ=(2.5, 1.2, 0.05, 0.02) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.09007, R̄=0.770, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.09933, R̄=0.765, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.12513, R̄=0.767, fail≈0.677, 1.1s\n  .. 1200/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.11603, R̄=0.768, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.08112, R̄=0.768, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(2.5, 1.2, 0.05, 0.02), avg util=-0.08889, R̄=0.769, fail≈0.688, 2.3s\nλ=(2.5, 1.2, 0.05, 0.02): util=-0.06894  R̄=0.769  fail≈0.683\nλ=(3.0, 1.2, 0.06, 0.03) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.26218, R̄=0.770, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.27223, R̄=0.765, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.30372, R̄=0.767, fail≈0.677, 1.2s\n  .. 1200/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.29289, R̄=0.768, fail≈0.682, 1.6s\n  .. 1500/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.25096, R̄=0.768, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(3.0, 1.2, 0.06, 0.03), avg util=-0.26055, R̄=0.769, fail≈0.688, 2.3s\nλ=(3.0, 1.2, 0.06, 0.03): util=-0.23676  R̄=0.768  fail≈0.683\nλ=(3.5, 1.4, 0.06, 0.03) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.42996, R̄=0.768, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.44094, R̄=0.764, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.47802, R̄=0.766, fail≈0.677, 1.2s\n  .. 1200/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.46543, R̄=0.767, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.41647, R̄=0.767, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(3.5, 1.4, 0.06, 0.03), avg util=-0.42791, R̄=0.768, fail≈0.688, 2.4s\nλ=(3.5, 1.4, 0.06, 0.03): util=-0.40007  R̄=0.768  fail≈0.683\nλ=(4.0, 1.6, 0.07, 0.04) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.60176, R̄=0.767, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.61360, R̄=0.762, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.65638, R̄=0.765, fail≈0.677, 1.1s\n  .. 1200/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.64211, R̄=0.766, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.58625, R̄=0.766, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(4.0, 1.6, 0.07, 0.04), avg util=-0.60050, R̄=0.767, fail≈0.688, 2.3s\nλ=(4.0, 1.6, 0.07, 0.04): util=-0.56853  R̄=0.767  fail≈0.683\nλ=(4.5, 1.6, 0.07, 0.04) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.76925, R̄=0.767, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.78348, R̄=0.762, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.83143, R̄=0.764, fail≈0.677, 1.1s\n  .. 1200/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.81520, R̄=0.765, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.75304, R̄=0.765, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(4.5, 1.6, 0.07, 0.04), avg util=-0.76899, R̄=0.766, fail≈0.688, 2.3s\nλ=(4.5, 1.6, 0.07, 0.04): util=-0.73286  R̄=0.766  fail≈0.683\nλ=(5.0, 1.8, 0.07, 0.05) → running backtest on 2000 frames ...\n  .. 300/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-0.93702, R̄=0.765, fail≈0.647, 0.4s\n  .. 600/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-0.95430, R̄=0.759, fail≈0.683, 0.8s\n  .. 900/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-1.00843, R̄=0.762, fail≈0.677, 1.1s\n  .. 1200/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-0.98979, R̄=0.763, fail≈0.682, 1.5s\n  .. 1500/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-0.92402, R̄=0.762, fail≈0.686, 1.9s\n  .. 1800/2000 frames for λ=(5.0, 1.8, 0.07, 0.05), avg util=-0.94187, R̄=0.764, fail≈0.688, 2.3s\nλ=(5.0, 1.8, 0.07, 0.05): util=-0.90158  R̄=0.763  fail≈0.683\nBest λ: (2.0, 1.0, 0.05, 0.02)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================== Cell 10. FINAL solver + save submission =====================\n",
    "R_GRID = np.round(np.arange(0.50, 0.901, 0.05), 2)\n",
    "base_head, kv8, kv16, ktrend, s_ceiling = best_params\n",
    "\n",
    "def H2(x):\n",
    "    x = np.clip(x, 1e-12, 1-1e-12)\n",
    "    return -(x*np.log2(x) + (1-x)*np.log2(1-x))\n",
    "\n",
    "def choose_candidates(e, fec, R_prev, v8, v16, trend_up, flags, head_override=None):\n",
    "    Rt = float(np.clip(1.0 - fec*H2(e), 0.0, 0.95))\n",
    "    hr = float(np.clip((head_override if head_override is not None\n",
    "                        else base_head + kv8*v8 + kv16*v16 + ktrend*(trend_up>0)), 0.004, 0.030))\n",
    "    safe = max(0.0, Rt - hr)\n",
    "\n",
    "    r_cands = R_GRID[R_GRID <= safe]\n",
    "    r0 = float(r_cands.max()) if len(r_cands) else 0.50\n",
    "\n",
    "    allow_up2 = (v8 < 0.007) and (v16 < 0.01) and (trend_up == 0) and (flags == 0)\n",
    "    if R_prev is not None:\n",
    "        j_prev = int(np.where(R_GRID==np.round(R_prev,2))[0][0])\n",
    "        j_cur  = int(np.where(R_GRID==np.round(r0,2))[0][0])\n",
    "        j_cur  = max(min(j_cur, j_prev+(2 if allow_up2 else 1)), j_prev-1)\n",
    "        r0     = float(R_GRID[j_cur])\n",
    "\n",
    "    j0   = int(np.where(R_GRID==np.round(r0,2))[0][0])\n",
    "    j_lo = max(0, j0-1); j_hi = min(len(R_GRID)-1, j0+1)\n",
    "    R_list = [float(R_GRID[j]) for j in range(j_lo, j_hi+1)]\n",
    "\n",
    "    cands = []\n",
    "    for Rv in R_list:\n",
    "        s_star = int(np.clip(round(32000*Rv - 27200*Rt), 0, 4800))\n",
    "        for ds in [0, +60, -60, +120, -120]:\n",
    "            s = int(np.clip(s_star + ds, 0, 4800))\n",
    "            p = 4800 - s\n",
    "            cands.append((Rv, s, p, Rt))\n",
    "    uniq = {}\n",
    "    for Rv,s,p,Rt_ in cands:\n",
    "        uniq[(Rv,s,p)] = Rt_\n",
    "    return [(R,s,p,Rt_) for (R,s,p),Rt_ in uniq.items()]\n",
    "\n",
    "def utility_for_candidate(e, fec, R_prev, row_bare, cand, lambdas):\n",
    "    lam1, lam2, lam3, lam4 = lambdas\n",
    "    R, s, p, Rt = cand\n",
    "    R_eff = float((32000*R - s) / 27200.0)\n",
    "    margin = R_eff - Rt\n",
    "\n",
    "    pf = fail_prob_predict(E=e, R=R, s=s, row_features=row_bare)\n",
    "    dR = 0.0 if R_prev is None else abs(R - R_prev)\n",
    "\n",
    "    util = (R\n",
    "            - lam1*pf\n",
    "            - lam2*max(0.0, margin)\n",
    "            - lam3*(s/4800.0)\n",
    "            - lam4*dR)\n",
    "    return util, pf, margin\n",
    "\n",
    "R_sel, s_sel, p_sel = [], [], []\n",
    "R_prev = None\n",
    "\n",
    "for i, idx in enumerate(target_idx):\n",
    "    row   = df.loc[idx]\n",
    "    e     = float(E_pred_final[i])\n",
    "    fec   = float(fec_hat[i])\n",
    "    v8    = float(row.get('E_mu_Z_std8', 0.0) or 0.0)\n",
    "    v16   = float(row.get('E_mu_Z_std16', 0.0) or 0.0)\n",
    "    tr    = float((row.get('E_mu_Z_diff1',0.0) or 0.0) > 0.0)\n",
    "    flg   = float((row.get('maintenance_flag',0)==1)\n",
    "                  + (row.get('synErr_prev',0) or 0 > 0)\n",
    "                  + (row.get('N_EC_prev',0) or 0 > 1))\n",
    "\n",
    "    cands = choose_candidates(e, fec, R_prev, v8, v16, tr, flg, head_override=None)\n",
    "\n",
    "    best = None\n",
    "    for cand in cands:\n",
    "        u, pf, m = utility_for_candidate(e, fec, R_prev, row, cand, LAM)\n",
    "        # adaptive pf cap: tighter on trouble frames\n",
    "        pf_cap = 0.08 + 0.04*np.clip(v8,0,0.03) + 0.02*np.clip(v16,0,0.03) + 0.03*(flg>0)\n",
    "        if pf <= pf_cap:\n",
    "            if (best is None) or (u > best[0]):\n",
    "                best = (u, pf, m, cand)\n",
    "\n",
    "    # if all over cap, pick overall best\n",
    "    if best is None:\n",
    "        for cand in cands:\n",
    "            u, pf, m = utility_for_candidate(e, fec, R_prev, row, cand, LAM)\n",
    "            if (best is None) or (u > best[0]):\n",
    "                best = (u, pf, m, cand)\n",
    "\n",
    "    R, s, p, _ = best[3]\n",
    "\n",
    "    # s ceiling safeguard with one-step R downshift if helps\n",
    "    if s >= s_ceiling:\n",
    "        pos = int(np.where(R_GRID==np.round(R,2))[0][0])\n",
    "        if pos>0:\n",
    "            R2 = float(R_GRID[pos-1])\n",
    "            Rt = float(np.clip(1.0 - fec*H2(e), 0.0, 0.95))\n",
    "            s2 = int(np.clip(round(32000*R2 - 27200*Rt), 0, 4800))\n",
    "            if s2 <= (s_ceiling - 60):\n",
    "                R, s = R2, s2\n",
    "                p    = 4800 - s\n",
    "\n",
    "    R_sel.append(np.round(R,2)); s_sel.append(int(s)); p_sel.append(int(p))\n",
    "    R_prev = R\n",
    "\n",
    "# --- Save submission ---\n",
    "out_path = \"/kaggle/working/submission.csv\"\n",
    "sub = pd.DataFrame({\n",
    "    0: [f\"{x:.16f}\" for x in E_pred_final],\n",
    "    1: [str(float(r)) for r in R_sel],\n",
    "    2: s_sel,\n",
    "    3: p_sel\n",
    "})\n",
    "sub.to_csv(out_path, header=False, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# --- Final checks ---\n",
    "loaded = pd.read_csv(out_path, header=None)\n",
    "assert loaded.shape == (2000,4)\n",
    "R_GRID = np.round(np.arange(0.50,0.901,0.05),2)\n",
    "assert np.all(np.isin(np.round(loaded[1].astype(float),2), R_GRID))\n",
    "assert np.all(loaded[2] + loaded[3] == 4800)\n",
    "print(\"OK: submission integrity checks passed\")\n",
    "print(loaded.head(10).to_string(index=False))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-11T22:02:26.684379Z",
     "iopub.execute_input": "2025-10-11T22:02:26.684675Z",
     "iopub.status.idle": "2025-10-11T22:03:06.056974Z",
     "shell.execute_reply.started": "2025-10-11T22:02:26.684656Z",
     "shell.execute_reply": "2025-10-11T22:03:06.056307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Saved: /kaggle/working/submission.csv\nOK: submission integrity checks passed\n       0    1    2    3\n0.025053 0.75 2000 2800\n0.021263 0.75 1382 3418\n0.020319 0.75 1229 3571\n0.019056 0.75 1035 3765\n0.020326 0.75 1324 3476\n0.026168 0.75 2220 2580\n0.026033 0.75 2184 2616\n0.025371 0.75 2167 2633\n0.028107 0.70 1024 3776\n0.022824 0.75 1577 3223\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
