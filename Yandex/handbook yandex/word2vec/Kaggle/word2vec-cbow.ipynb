{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7088345,"sourceType":"datasetVersion","datasetId":4084268,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport re\nimport itertools\nimport kagglehub\nimport os\nimport pandas as pd\nimport numpy as np\nimport random\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nprint(\"Torch:\", torch.__version__)\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:17.717441Z","iopub.execute_input":"2025-09-13T21:17:17.718221Z","iopub.status.idle":"2025-09-13T21:17:22.711205Z","shell.execute_reply.started":"2025-09-13T21:17:17.718192Z","shell.execute_reply":"2025-09-13T21:17:22.710142Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load dataset with Russian Jokes","metadata":{}},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"vsevolodbogodist/data-jokes\")\nprint(\"Path to dataset files:\", path)\ncsv_path = os.path.join(path, \"dataset.csv\")\ndf = pd.read_csv(csv_path, sep=\",\", quotechar='\"')\nprint(df.head())\n\ntexts = df[\"text\"].astype(str).tolist()\ntexts = texts[:100000]\nprint(len(texts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:22.712157Z","iopub.execute_input":"2025-09-13T21:17:22.712593Z","iopub.status.idle":"2025-09-13T21:17:24.380863Z","shell.execute_reply.started":"2025-09-13T21:17:22.712571Z","shell.execute_reply":"2025-09-13T21:17:24.379628Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-jokes\n                                                text\n0  - Зять, а ты знаешь, где найти того мужчину, к...\n1  После проведения акции \"К животным по-человече...\n2  Штирлиц пришел домой и сразу завалился на боко...\n3  Комету нашли русские, а захватила ее Европа. И...\n4  - Мальчик, какой у тебя огромный рюкзачок, что...\n100000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Токенизация и словарь\n","metadata":{}},{"cell_type":"code","source":"def tokenize(s):\n    s = re.sub(r\"[^а-я ]+\", \"\", s.lower())\n    return s.split()\n\ntokens_list = [tokenize(t) for t in texts]\ntokens = list(itertools.chain.from_iterable(tokens_list))\n\nvocab = list(set(tokens))\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\nV = len(vocab)\nprint(\"Размер словаря:\", V)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:24.382900Z","iopub.execute_input":"2025-09-13T21:17:24.383165Z","iopub.status.idle":"2025-09-13T21:17:26.261057Z","shell.execute_reply.started":"2025-09-13T21:17:24.383146Z","shell.execute_reply":"2025-09-13T21:17:26.260382Z"}},"outputs":[{"name":"stdout","text":"Размер словаря: 175504\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def generate_cbow_pairs(tokens, window_size=6):\n    pairs = []\n    for center_pos in range(window_size, len(tokens) - window_size):\n        context = (\n            tokens[center_pos - window_size:center_pos] +\n            tokens[center_pos + 1:center_pos + window_size + 1]\n        )\n        target = tokens[center_pos]\n        pairs.append((context, target))\n    return pairs\n\npairs = generate_cbow_pairs(tokens, window_size=6)\ntraining_data = [([word2idx[w] for w in context], word2idx[target]) for context, target in pairs]\nprint(\"Количество пар:\", len(training_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:26.261919Z","iopub.execute_input":"2025-09-13T21:17:26.262191Z","iopub.status.idle":"2025-09-13T21:17:39.410175Z","shell.execute_reply.started":"2025-09-13T21:17:26.262161Z","shell.execute_reply":"2025-09-13T21:17:39.409296Z"}},"outputs":[{"name":"stdout","text":"Количество пар: 2356463\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## параметры модели\n","metadata":{}},{"cell_type":"code","source":"class CBOWModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(CBOWModel, self).__init__()\n        # \"input\" и \"output\" эмбеддинги (как в word2vec)\n        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, context_idxs):\n        \"\"\"\n        context_idxs: LongTensor [batch_size, context_len]\n        \"\"\"\n        # 1. эмбеддинги всех слов контекста\n        context_vecs = self.in_embed(context_idxs)        # [batch, context_len, D]\n        v_context = context_vecs.mean(dim=1)              # [batch, D]\n\n        # 2. логиты = скалярные произведения со всеми \"output embeddings\"\n        #   out_embed.weight имеет shape [V, D]\n        logits = torch.matmul(v_context, self.out_embed.weight.t())  # [batch, V]\n\n        # 3. softmax будет применён внутри CrossEntropyLoss\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:39.411123Z","iopub.execute_input":"2025-09-13T21:17:39.411334Z","iopub.status.idle":"2025-09-13T21:17:39.417806Z","shell.execute_reply.started":"2025-09-13T21:17:39.411318Z","shell.execute_reply":"2025-09-13T21:17:39.416809Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"embedding_dim = 50\nmodel = CBOWModel(V, embedding_dim).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\nepochs = 10\nbatch_size = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:39.418574Z","iopub.execute_input":"2025-09-13T21:17:39.418893Z","iopub.status.idle":"2025-09-13T21:17:42.494774Z","shell.execute_reply.started":"2025-09-13T21:17:39.418865Z","shell.execute_reply":"2025-09-13T21:17:42.493791Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Обучение\n","metadata":{}},{"cell_type":"code","source":"def get_batch(data, batch_size, device):\n    batch = random.sample(data, batch_size)\n    contexts, targets = zip(*batch)\n    contexts_tensor = torch.tensor(list(contexts), dtype=torch.long, device=device)\n    targets_tensor = torch.tensor(list(targets), dtype=torch.long, device=device)\n    return contexts_tensor, targets_tensor\n\n\nfor epoch in range(epochs):\n    total_loss = 0\n    steps = len(training_data) // batch_size\n    for _ in range(steps):\n        contexts, targets = get_batch(training_data, batch_size, device)\n\n        logits = model(contexts)                # [batch, V]\n        loss = criterion(logits, targets)       # CE: softmax+NLL внутри\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Эпоха {epoch+1}, loss = {total_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:17:42.495760Z","iopub.execute_input":"2025-09-13T21:17:42.496232Z","iopub.status.idle":"2025-09-13T21:29:46.880620Z","shell.execute_reply.started":"2025-09-13T21:17:42.496208Z","shell.execute_reply":"2025-09-13T21:29:46.879945Z"}},"outputs":[{"name":"stdout","text":"Эпоха 1, loss = 130540.3924\nЭпоха 2, loss = 128897.1386\nЭпоха 3, loss = 127759.2010\nЭпоха 4, loss = 126768.4041\nЭпоха 5, loss = 125720.7439\nЭпоха 6, loss = 124525.1913\nЭпоха 7, loss = 123137.3921\nЭпоха 8, loss = 121722.9296\nЭпоха 9, loss = 120561.6587\nЭпоха 10, loss = 119529.0292\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Сохранение","metadata":{}},{"cell_type":"code","source":"torch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"word2idx\": word2idx,\n    \"idx2word\": idx2word,\n    \"embedding_dim\": embedding_dim,\n    \"vocab_size\": V\n}, \"word2vec_CBOW.pth\")\n\nprint(\"model saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:29:46.881350Z","iopub.execute_input":"2025-09-13T21:29:46.881544Z","iopub.status.idle":"2025-09-13T21:29:47.414205Z","shell.execute_reply.started":"2025-09-13T21:29:46.881521Z","shell.execute_reply":"2025-09-13T21:29:47.413328Z"}},"outputs":[{"name":"stdout","text":"model saved\n","output_type":"stream"}],"execution_count":8}]}