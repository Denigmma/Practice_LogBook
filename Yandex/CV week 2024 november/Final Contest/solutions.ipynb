{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Week: Итоговое задание по Consistency Distillation\n",
    "\n",
    "В этом ноутбуке представлены решения заданий контеста по дистилляции многошаговой диффузионной модели в малошагового студента с использованием Consistency Distillation. Мы будем использовать модель Stable Diffusion 1.5 (SD1.5) для генерации изображений по текстовому описанию и применим различные техники для оптимизации процесса обучения и генерации.\n",
    "\n",
    "## Содержание\n",
    "1. Загрузка и настройка модели Stable Diffusion 1.5\n",
    "2. Реализация шага DDIM\n",
    "3. Consistency Training\n",
    "4. Multi-boundary timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №1: Загрузка модели Stable Diffusion 1.5\n",
    "\n",
    "Для начала загрузим модель Stable Diffusion 1.5 и сгенерируем изображения за 50 шагов. Все компоненты модели будут загружены в формате FP16 для экономии памяти, и модель будет размещена на GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install diffusers==0.30.3 peft==0.8.2 huggingface_hub==0.23.4\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "# Загрузка модели Stable Diffusion 1.5\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Проверяем, что все компоненты модели в FP16 и на cuda\n",
    "assert pipe.unet.dtype == torch.float16 and pipe.unet.device.type == 'cuda'\n",
    "assert pipe.vae.dtype == torch.float16 and pipe.vae.device.type == 'cuda'\n",
    "assert pipe.text_encoder.dtype == torch.float16 and pipe.text_encoder.device.type == 'cuda'\n",
    "\n",
    "# Заменяем дефолтный сэмплер на DDIM\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "pipe.scheduler.timesteps = pipe.scheduler.timesteps.cuda()\n",
    "pipe.scheduler.alphas_cumprod = pipe.scheduler.alphas_cumprod.cuda()\n",
    "\n",
    "# Отдельно извлекаем модель учителя, которую потом будем дистиллировать\n",
    "teacher_unet = pipe.unet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сгенерируем изображения за 50 шагов и за 4 шага, чтобы увидеть разницу в качестве.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Вспомогательная функция для визуализации изображений\n",
    "def visualize_images(images):\n",
    "    assert len(images) == 4\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(wspace=-0.01, hspace=-0.01)\n",
    "    plt.show()\n",
    "\n",
    "# Генерация изображений за 50 шагов\n",
    "prompt = \"A sad puppy with large eyes\"\n",
    "guidance_scale = 7.5\n",
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_images_per_prompt=4\n",
    ").images\n",
    "\n",
    "visualize_images(images)\n",
    "\n",
    "# Генерация изображений за 4 шага\n",
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=4,\n",
    "    generator=generator,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_images_per_prompt=4\n",
    ").images\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, при уменьшении числа шагов до 4 изображение становится менее четким. Далее мы постараемся улучшить качество генерации при помощи Consistency Training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №2: Реализация шага DDIM\n",
    "\n",
    "Реализуем функцию `ddim_solver_step`, которая выполняет один шаг DDIM-солвера для перехода от шага \\( t \\) к шагу \\( s \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def ddim_solver_step(model_output, x_t, t, s, scheduler):\n",
    "    \"\"\"\n",
    "    Реализация шага DDIM солвера для VP процесса зашумления и eps-prediction модели.\n",
    "\n",
    "    Params:\n",
    "        model_output: torch.Tensor[B, 4, 64, 64] - предсказание модели - шум ε\n",
    "        x_t: torch.Tensor[B, 4, 64, 64] - сэмплы на шаге t\n",
    "        t: torch.Tensor[B] - номер текущего шага\n",
    "        s: torch.Tensor[B] - номер следующего шага\n",
    "        scheduler: DDIMScheduler - расписание диффузионного процесса, чтобы получить alpha и sigma\n",
    "    \"\"\"\n",
    "    alphas_cumprod = scheduler.alphas_cumprod.to(x_t.device)\n",
    "    alphas = torch.sqrt(alphas_cumprod)  # α_t = sqrt(ᾱ_t)\n",
    "    sigmas = torch.sqrt(1.0 - alphas_cumprod)  # σ_t = sqrt(1 - ᾱ_t)\n",
    "\n",
    "    sigmas_s = extract_into_tensor(sigmas, s, x_t.shape)\n",
    "    alphas_s = extract_into_tensor(alphas, s, x_t.shape)\n",
    "\n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x_t.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x_t.shape)\n",
    "\n",
    "    alphas_s[s == 0] = 1.0\n",
    "    sigmas_s[s == 0] = 0.0\n",
    "\n",
    "    alphas_t[t == 0] = 1.0\n",
    "    sigmas_t[t == 0] = 0.0\n",
    "\n",
    "    x_0 = (x_t - sigmas_t * model_output) / alphas_t\n",
    "\n",
    "    x_s = alphas_s * x_0 + sigmas_s * model_output\n",
    "\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `ddim_solver_step` реализует один шаг перехода от шага \\( t \\) к шагу \\( s \\) с использованием предсказанного шума модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №3: Consistency Training\n",
    "\n",
    "### Задание №3.1: Деривация аналитического перехода\n",
    "\n",
    "Мы стремимся выразить \\( \\mathbf{x}_s \\) через \\( \\mathbf{x}_t \\) и \\( \\mathbf{x}_0 \\) аналитически, используя формулы DDIM и оценку скор функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "### Деривация\n",
    "\n",
    "Начнем с формулы зашумления:\n",
    "\\[\n",
    "\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon_\\theta\n",
    "\\]\n",
    "где \\( \\epsilon_\\theta \\sim \\mathcal{N}(0, I) \\).\n",
    "\n",
    "Выразим \\( \\epsilon_\\theta \\):\n",
    "\\[\n",
    "\\epsilon_\\theta = \\frac{\\mathbf{x}_t - \\alpha_t \\mathbf{x}_0}{\\sigma_t}\n",
    "\\]\n",
    "\n",
    "Используя формулу DDIM для перехода:\n",
    "\\[\n",
    "\\mathbf{x}_s = \\alpha_s \\mathbf{x}_0 + \\sigma_s \\epsilon_\\theta\n",
    "\\]\n",
    "\n",
    "Подставляем выражение для \\( \\epsilon_\\theta \\):\n",
    "\\[\n",
    "\\mathbf{x}_s = \\alpha_s \\mathbf{x}_0 + \\sigma_s \\left( \\frac{\\mathbf{x}_t - \\alpha_t \\mathbf{x}_0}{\\sigma_t} \\right)\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{x}_s = \\alpha_s \\mathbf{x}_0 + \\frac{\\sigma_s}{\\sigma_t} \\mathbf{x}_t - \\frac{\\sigma_s \\alpha_t}{\\sigma_t} \\mathbf{x}_0\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{x}_s = \\left( \\alpha_s - \\frac{\\sigma_s \\alpha_t}{\\sigma_t} \\right) \\mathbf{x}_0 + \\frac{\\sigma_s}{\\sigma_t} \\mathbf{x}_t\n",
    "\\]\n",
    "\n",
    "Таким образом, функция `get_xs_from_xt_naive` вычисляет \\( \\mathbf{x}_s \\) аналитически.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def get_xs_from_xt_naive(\n",
    "    x_0, x_t, t, s,  # Не все эти аргументы могут быть вам нужны\n",
    "    scheduler,\n",
    "    noise=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Получение точки x_s в CT режиме, т.е., аналитически.\n",
    "    \n",
    "    Params:\n",
    "        x_0: torch.Tensor[B, C, H, W] - чистые данные x0\n",
    "        x_t: torch.Tensor[B, C, H, W] - зашумленные данные на шаге t\n",
    "        t: torch.Tensor[B] - номера текущих шагов\n",
    "        s: torch.Tensor[B] - номера следующих шагов\n",
    "        scheduler: DDIMScheduler - расписание диффузионного процесса\n",
    "        noise: torch.Tensor[B, C, H, W] - шум ε (опционально)\n",
    "        \n",
    "    Returns:\n",
    "        x_s: torch.Tensor[B, C, H, W] - данные на шаге s\n",
    "    \"\"\"\n",
    "    # Извлекаем alphas_cumprod и вычисляем alpha и sigma\n",
    "    alphas_cumprod = scheduler.alphas_cumprod.to(x_t.device)\n",
    "    alphas = torch.sqrt(alphas_cumprod)  # α_t = sqrt(ᾱ_t)\n",
    "    sigmas = torch.sqrt(1.0 - alphas_cumprod)  # σ_t = sqrt(1 - ᾱ_t)\n",
    "    \n",
    "    # Извлекаем α_s, σ_s и α_t, σ_t для соответствующих шагов\n",
    "    sigmas_s = extract_into_tensor(sigmas, s, x_t.shape)\n",
    "    alphas_s = extract_into_tensor(alphas, s, x_t.shape)\n",
    "    \n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x_t.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x_t.shape)\n",
    "    \n",
    "    # Устанавливаем граничные условия\n",
    "    alphas_s[s == 0] = 1.0\n",
    "    sigmas_s[s == 0] = 0.0\n",
    "    \n",
    "    alphas_t[t == 0] = 1.0\n",
    "    sigmas_t[t == 0] = 0.0\n",
    "    \n",
    "    # Вычисляем ε_theta\n",
    "    epsilon_theta = (x_t - alphas_t * x_0) / sigmas_t\n",
    "    \n",
    "    # Вычисляем x_s по формуле DDIM\n",
    "    x_s = alphas_s * x_0 + sigmas_s * epsilon_theta\n",
    "    \n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3.2: Реализация функции расчета лосса\n",
    "\n",
    "Теперь реализуем функцию `cm_loss_template`, которая рассчитывает лосс для консистенси моделей.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "\n",
    "def cm_loss_template(\n",
    "    latents, prompt_embeds,  # батч латентов и текстовых эмбедов\n",
    "    unet, scheduler,\n",
    "\n",
    "    # Функции, которые будем постепенно менять из задания к заданию\n",
    "    loss_fn: callable,\n",
    "    get_boundary_timesteps: callable,\n",
    "    get_xs_from_xt: callable,\n",
    "\n",
    "    num_timesteps=1000,\n",
    "    step_size=20,  # Указываем с каким интервалом берем шаги s и t.\n",
    "):\n",
    "    # Сэмплируем случайные шаги t для каждого элемента батча t ~ U[step_size-1, 999]\n",
    "    assert num_timesteps == 1000\n",
    "    num_intervals = num_timesteps // step_size\n",
    "\n",
    "    index = torch.randint(1, num_intervals, (len(latents),), device=latents.device).long()  # [1, num_intervals]\n",
    "    t = step_size * index - 1\n",
    "    s = torch.clamp(t - step_size, min=0)\n",
    "    boundary_timesteps = get_boundary_timesteps(\n",
    "        s, num_timesteps=num_timesteps\n",
    "    )\n",
    "\n",
    "    # Сэмплируем x_t\n",
    "    noise = torch.randn_like(latents)\n",
    "    x_t = q_sample(latents, t, scheduler, noise=noise)\n",
    "\n",
    "    # Для реализации mixed-precision обучения\n",
    "    with torch.cuda.amp.autocast():\n",
    "        noise_pred = unet(x_t.float(), t, encoder_hidden_states=prompt_embeds.float()).sample\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_t\n",
    "    boundary_pred = unet(x_t.float(), boundary_timesteps, encoder_hidden_states=prompt_embeds.float()).sample\n",
    "\n",
    "    # Получаем сэмпл x_s из x_t\n",
    "    x_s = get_xs_from_xt(\n",
    "        latents, x_t, t, s,\n",
    "        scheduler,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        noise=noise,\n",
    "    )\n",
    "\n",
    "    # Предсказание \"таргет моделью\"\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        target_noise_pred = unet(x_s.float(), s, encoder_hidden_states=prompt_embeds.float()).sample\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_s\n",
    "    boundary_target = unet(x_s.float(), boundary_timesteps, encoder_hidden_states=prompt_embeds.float()).sample\n",
    "\n",
    "    loss = loss_fn(boundary_pred, boundary_target)\n",
    "    return loss\n",
    "\n",
    "def get_zero_boundary_timesteps(t, **kwargs):\n",
    "    \"\"\"\n",
    "    Определяем шаги где будут срабатывать граничные условия.\n",
    "    Для классических СM это t=0.\n",
    "    \"\"\"\n",
    "    return torch.zeros_like(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `cm_loss_template` рассчитывает лосс для консистенси моделей, учитывая граничные условия и предсказания модели на разных шагах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №4: Multi-boundary timesteps\n",
    "\n",
    "Теперь реализуем функцию `get_multi_boundary_timesteps`, которая определяет граничные точки для разделения траекторий на несколько отрезков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_multi_boundary_timesteps(timesteps, num_boundaries=4, num_timesteps=1000):\n",
    "    \"\"\"\n",
    "    Для батча таймстепов определяет соответствующие граничные точки.\n",
    "    \n",
    "    params:\n",
    "        timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "        num_boundaries (int): Количество граничных точек (отрезков)\n",
    "        num_timesteps (int): Общее количество таймстепов\n",
    "    \n",
    "    returns:\n",
    "        boundary_timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "    \"\"\"\n",
    "    # Вычисляем шаг между границами\n",
    "    step = num_timesteps // num_boundaries  # Например, 1000 // 4 = 250\n",
    "    \n",
    "    # Создаем тензор границ\n",
    "    boundaries = torch.arange(0, num_timesteps, step, device=timesteps.device)\n",
    "    # Для num_boundaries=4 и num_timesteps=1000 получим boundaries = [0, 250, 500, 750]\n",
    "    \n",
    "    # Используем torch.bucketize для нахождения индекса границы для каждого t\n",
    "    # right=False означает, что граница включается в нижний интервал\n",
    "    boundary_indices = torch.bucketize(timesteps, boundaries, right=False) - 1\n",
    "    \n",
    "    # Убеждаемся, что индексы не выходят за пределы\n",
    "    boundary_indices = boundary_indices.clamp(min=0)\n",
    "    \n",
    "    # Получаем соответствующие границы\n",
    "    boundary_timesteps = boundaries[boundary_indices]\n",
    "    \n",
    "    return boundary_timesteps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `get_multi_boundary_timesteps` разделяет траекторию диффузионного процесса на несколько отрезков и определяет соответствующую граничную точку для каждого таймстепа в батче.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
