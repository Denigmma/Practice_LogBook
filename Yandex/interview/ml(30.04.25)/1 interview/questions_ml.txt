Берем бинарную классификацию, напишите для них алгоритм,
 формулы - которые будете использовать, объясните, что происходит.

Есть такие значения:
x = (x1,x2,…,xm) \in Rm; y \in {0, 1} ; D = {(x,y)}; n = len(D)


формула (функция алгоритма):

h(x) =P(y=1|x,w,b)=sigmoid(w^Tx+b) sigmod(t)=1/(1+e^-t)


лосс, функционал:

loss =- (1/m)SUM[i=1,m](yi*ln(h)+(1-yi)*ln(1-h))

entropy(ksi) = - sum p_i * log(pi)

ksi - случ величина


если y (target) это вероятность (клика = клики / показы) y \in [0,1]

какой лосс нужен в такой ситуации?

та же кросс энтропия, менять ничего не нужно.

метод оптимизации: град спуск.

dloss/dw=(1/m)SUM(sigmoid(w^Tx+b)-yi)

w=w-alpha*dloss/dw


условия работы алгоритма:

1) существование производной
2)...
(тут недочет, нужно было больше рассказать)



допустим есть два алгоритма: baseline, new_method

как бы выглядел пайплайн для сравнения качества двух моделей.


посмотреть на метрики, сравнить на кросс валидации.

D = датасет, надо его разбить на фолды.

d_i формируется

k fold, делим на k частей. данные разбиваются без пересечений, одинакового размера.

train / test/ val split

нужны сами метрики:accuracy, precision, recall, f1, focal loss

f1 = 2 *(p*r)/(p+r)

замеряем кросс энтропию на разных фолдах.

ce(baseline, test) = alpha; ce(new_method, test) = beta

alpha < beta?

новый метод выиграл, покатим в прод? На что мы должны смотреть, на что обратить внимание?

f1(baseline) = 0.8; f1(new_method) = 0.81

катим в прод?



Комментарии по интервью:

(00:00):
 Тут частный случай, крайний случай про пустой список, действительно его надо обрабатывать. Часть решения содержит копипасту, что минор, но не очень хорошо. Это тоже оценивается.  И вот эти множественные прислания, они, ну, вкусовщина, но, честно говоря, тоже минорное замечание.  Остальное все отлично.

(00:30):
 По поводу линейного решения предлагаю подумать на досуге, и я думаю, что это, вообще говоря, полезно.  На секции MLP это не имеет никакого значения.  Вернее, если бы вы придумали алгоритм, то это был бы большой плюс.  В остальном, я предлагаю в качестве тренировки дома подумать.

(00:58):
 Опыт у ML.  Про математику здесь вопросов нет.  Значит, у формула записано верно, лосс записан верно.  Там отпечатки бывают у всех.  Здесь вопросов нет, энтропия окей, вспомнилась с трудом, но да.

(01:21):
 Про то, что вы понимаете, что вероятностный лосс оптимизируется при переходе на непрерывный таргет, не переходит в задачу массы, это отлично.  Дальше я там времени не стал тратить, хотя тут можно еще покопаться.  По поводу метода оптимизации здесь записано все верно, но немножечко про условия работы алгоритма здесь хотелось бы чуть побольше услышать.

(01:52):
 Я думаю, что стоит прочитать про какие-то практические вопросы и как условия применения.  Здесь хотелось бы услышать, в частности, например, когда мы достигаем глобального минимума, когда он нам гарантируется.  Я подозреваю, что вы это знаете.  Имеется в виду про ограничения, типа чтобы не искать градиент, когда он там близок к нулю, а смотреть до определенного какого-то Эпсилона?

(02:21):
 Это тоже можно обозначить, но здесь больше есть условия Робина Карпа, например, вот их можно почитать.  Скажем так. Ну и условия сходимости в глобальные оптимумы. В каких случаях оно сходится в глобальные оптимумы, а не в локальные.

(02:46):
 По поводу сравнения двух алгоритмов, здесь вы все вроде бы правильно рассказываете.  вот так метрика ok сравнение здесь этот момент немножечко выдвинулась из колеи  когда пока темп рот когда-нибудь покрасим я советую посчитать здесь информацию о A/B тестировании и достаточно внимательно прочитать часть методы статистической оценки, сравнения  гипотез. В общем, когда у нас есть контрольная выборка, например, когда есть тестовая выборка,  что происходит в таких ситуациях и как принимается решение наверное как-то конкретного ответа я к  сожалению здесь дать не могу но могу вот как бы посоветовать есть яндекс учебник по e-mail там  есть достаточно подробное описание по сравнению качества мне кажется что там достаточно неплохо  описано, и есть ссылки на разные статьи, которые тоже можно было бы почитать, и кажется, что они могут быть полезны.

(04:15):
 По поводу, кстати, градиентного спуска тоже можно там же прочитать.  Остальное, вроде бы я все рассказал, если еще какие-то вопросы остались, то как-то так.

(04:31):
 Финальная оценка, я считаю, что секция пройдена. Детали, как она пройдена, я раскрыть не могу. Это уже получит информацию рекрутер. Но вы можете сделать выводы из моего фидбэка.  Спасибо вам огромное.






